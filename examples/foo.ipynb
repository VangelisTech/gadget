{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "import daft \n",
    "import inspect\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class DataFrame:\n",
      "    \"\"\"A Daft DataFrame is a table of data.\n",
      "\n",
      "    It has columns, where each column has a type and the same\n",
      "    number of items (rows) as all other columns.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, builder: LogicalPlanBuilder) -> None:\n",
      "        \"\"\"Constructs a DataFrame according to a given LogicalPlan.\n",
      "\n",
      "        Users are expected instead to call\n",
      "        the classmethods on DataFrame to create a DataFrame.\n",
      "\n",
      "        Args:\n",
      "            plan: LogicalPlan describing the steps required to arrive at this DataFrame\n",
      "        \"\"\"\n",
      "        if not isinstance(builder, LogicalPlanBuilder):\n",
      "            if isinstance(builder, dict):\n",
      "                raise ValueError(\n",
      "                    \"DataFrames should be constructed with a dictionary of columns using `daft.from_pydict`\"\n",
      "                )\n",
      "            if isinstance(builder, list):\n",
      "                raise ValueError(\n",
      "                    \"DataFrames should be constructed with a list of dictionaries using `daft.from_pylist`\"\n",
      "                )\n",
      "            raise ValueError(f\"Expected DataFrame to be constructed with a LogicalPlanBuilder, received: {builder}\")\n",
      "\n",
      "        self.__builder = builder\n",
      "        self._result_cache: Optional[PartitionCacheEntry] = None\n",
      "        self._preview = DataFramePreview(preview_partition=None, dataframe_num_rows=None)\n",
      "        self._num_preview_rows = get_context().daft_execution_config.num_preview_rows\n",
      "\n",
      "    @property\n",
      "    def _builder(self) -> LogicalPlanBuilder:\n",
      "        if self._result_cache is None:\n",
      "            return self.__builder\n",
      "        else:\n",
      "            num_partitions = self._result_cache.num_partitions()\n",
      "            size_bytes = self._result_cache.size_bytes()\n",
      "            num_rows = self._result_cache.num_rows()\n",
      "\n",
      "            # Partition set should always be set on cache entry.\n",
      "            assert (\n",
      "                num_partitions is not None and size_bytes is not None and num_rows is not None\n",
      "            ), \"Partition set should always be set on cache entry\"\n",
      "\n",
      "            return self.__builder.from_in_memory_scan(\n",
      "                self._result_cache,\n",
      "                self.__builder.schema(),\n",
      "                num_partitions=num_partitions,\n",
      "                size_bytes=size_bytes,\n",
      "                num_rows=num_rows,\n",
      "            )\n",
      "\n",
      "    def _get_current_builder(self) -> LogicalPlanBuilder:\n",
      "        \"\"\"Returns the current logical plan builder, without any caching optimizations.\"\"\"\n",
      "        return self.__builder\n",
      "\n",
      "    @property\n",
      "    def _result(self) -> Optional[PartitionSet]:\n",
      "        if self._result_cache is None:\n",
      "            return None\n",
      "        else:\n",
      "            return self._result_cache.value\n",
      "\n",
      "    def _broadcast_query_plan(self, plan_time_start: datetime, plan_time_end: datetime):\n",
      "        from daft import dashboard\n",
      "        from daft.dataframe.display import MermaidFormatter\n",
      "\n",
      "        if not dashboard._should_run():\n",
      "            return\n",
      "\n",
      "        is_cached = self._result_cache is not None\n",
      "        mermaid_plan: str = MermaidFormatter(\n",
      "            builder=self.__builder,\n",
      "            show_all=True,\n",
      "            simple=False,\n",
      "            is_cached=is_cached,\n",
      "        )._repr_markdown_()\n",
      "\n",
      "        dashboard.broadcast_query_information(\n",
      "            mermaid_plan=mermaid_plan,\n",
      "            plan_time_start=plan_time_start,\n",
      "            plan_time_end=plan_time_end,\n",
      "        )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def explain(\n",
      "        self, show_all: bool = False, format: str = \"ascii\", simple: bool = False, file: Optional[io.IOBase] = None\n",
      "    ) -> Any:\n",
      "        \"\"\"Prints the (logical and physical) plans that will be executed to produce this DataFrame.\n",
      "\n",
      "        Defaults to showing the unoptimized logical plan. Use ``show_all=True`` to show the unoptimized logical plan,\n",
      "        the optimized logical plan, and the physical plan.\n",
      "\n",
      "        Args:\n",
      "            show_all (bool): Whether to show the optimized logical plan and the physical plan in addition to the\n",
      "                unoptimized logical plan.\n",
      "            format (str): The format to print the plan in. one of 'ascii' or 'mermaid'\n",
      "            simple (bool): Whether to only show the type of op for each node in the plan, rather than showing details\n",
      "                of how each op is configured.\n",
      "\n",
      "            file (Optional[io.IOBase]): Location to print the output to, or defaults to None which defaults to the default location for\n",
      "                print (in Python, that should be sys.stdout)\n",
      "        \"\"\"\n",
      "        is_cached = self._result_cache is not None\n",
      "        if format == \"mermaid\":\n",
      "            from daft.dataframe.display import MermaidFormatter\n",
      "            from daft.utils import in_notebook\n",
      "\n",
      "            instance = MermaidFormatter(self.__builder, show_all, simple, is_cached)\n",
      "            if file is not None:\n",
      "                # if we are printing to a file, we print the markdown representation of the plan\n",
      "                text = instance._repr_markdown_()\n",
      "                print(text, file=file)\n",
      "            if in_notebook():\n",
      "                # if in a notebook, we return the class instance and let jupyter display it\n",
      "                return instance\n",
      "            else:\n",
      "                # if we are not in a notebook, we return the raw markdown instead of the class instance\n",
      "                return repr(instance)\n",
      "\n",
      "        print_to_file = partial(print, file=file)\n",
      "\n",
      "        if self._result_cache is not None:\n",
      "            print_to_file(\"Result is cached and will skip computation\\n\")\n",
      "            print_to_file(self._builder.pretty_print(simple, format=format))\n",
      "\n",
      "            print_to_file(\"However here is the logical plan used to produce this result:\\n\", file=file)\n",
      "\n",
      "        builder = self.__builder\n",
      "        print_to_file(\"== Unoptimized Logical Plan ==\\n\")\n",
      "        print_to_file(builder.pretty_print(simple, format=format))\n",
      "        if show_all:\n",
      "            print_to_file(\"\\n== Optimized Logical Plan ==\\n\")\n",
      "            builder = builder.optimize()\n",
      "            print_to_file(builder.pretty_print(simple))\n",
      "            print_to_file(\"\\n== Physical Plan ==\\n\")\n",
      "            if get_context().get_or_create_runner().name != \"native\":\n",
      "                physical_plan_scheduler = builder.to_physical_plan_scheduler(get_context().daft_execution_config)\n",
      "                print_to_file(physical_plan_scheduler.pretty_print(simple, format=format))\n",
      "            else:\n",
      "                native_executor = NativeExecutor()\n",
      "                print_to_file(\n",
      "                    native_executor.pretty_print(builder, get_context().daft_execution_config, simple, format=format)\n",
      "                )\n",
      "        else:\n",
      "            print_to_file(\n",
      "                \"\\n \\nSet `show_all=True` to also see the Optimized and Physical plans. This will run the query optimizer.\",\n",
      "            )\n",
      "        return None\n",
      "\n",
      "    def num_partitions(self) -> int:\n",
      "        # We need to run the optimizer since that could change the number of partitions\n",
      "        return (\n",
      "            self.__builder.optimize().to_physical_plan_scheduler(get_context().daft_execution_config).num_partitions()\n",
      "        )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def schema(self) -> Schema:\n",
      "        \"\"\"Returns the Schema of the DataFrame, which provides information about each column, as a Python object.\n",
      "\n",
      "        Returns:\n",
      "            Schema: schema of the DataFrame\n",
      "        \"\"\"\n",
      "        return self.__builder.schema()\n",
      "\n",
      "    @property\n",
      "    def column_names(self) -> List[str]:\n",
      "        \"\"\"Returns column names of DataFrame as a list of strings.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: Column names of this DataFrame.\n",
      "        \"\"\"\n",
      "        return self.__builder.schema().column_names()\n",
      "\n",
      "    @property\n",
      "    def columns(self) -> List[Expression]:\n",
      "        \"\"\"Returns column of DataFrame as a list of Expressions.\n",
      "\n",
      "        Returns:\n",
      "            List[Expression]: Columns of this DataFrame.\n",
      "        \"\"\"\n",
      "        return [col(field.name) for field in self.__builder.schema()]\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def __iter__(self) -> Iterator[Dict[str, Any]]:\n",
      "        \"\"\"Alias of `self.iter_rows()` with default arguments for convenient access of data.\"\"\"\n",
      "        return self.iter_rows(results_buffer_size=None)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def iter_rows(\n",
      "        self,\n",
      "        results_buffer_size: Union[Optional[int], Literal[\"num_cpus\"]] = \"num_cpus\",\n",
      "        column_format: Literal[\"python\", \"arrow\"] = \"python\",\n",
      "    ) -> Iterator[Dict[str, Any]]:\n",
      "        \"\"\"Return an iterator of rows for this dataframe.\n",
      "\n",
      "        Each row will be a Python dictionary of the form { \"key\" : value, ... }. If you are instead looking to iterate over\n",
      "        entire partitions of data, see: :meth:`df.iter_partitions() <daft.DataFrame.iter_partitions>`.\n",
      "\n",
      "        By default, Daft will convert the columns to Python lists for easy consumption. Datatypes with Python equivalents will be converted accordingly, e.g. timestamps to datetime, tensors to numpy arrays.\n",
      "        For nested data such as List or Struct arrays, however, this can be expensive. You may wish to set `column_format` to \"arrow\" such that the nested data is returned as Arrow scalars.\n",
      "\n",
      "        .. NOTE::\n",
      "            A quick note on configuring asynchronous/parallel execution using `results_buffer_size`.\n",
      "\n",
      "            The `results_buffer_size` kwarg controls how many results Daft will allow to be in the buffer while iterating.\n",
      "            Once this buffer is filled, Daft will not run any more work until some partition is consumed from the buffer.\n",
      "\n",
      "            * Increasing this value means the iterator will consume more memory and CPU resources but have higher throughput\n",
      "            * Decreasing this value means the iterator will consume lower memory and CPU resources, but have lower throughput\n",
      "            * Setting this value to `None` means the iterator will consume as much resources as it deems appropriate per-iteration\n",
      "\n",
      "            The default value is the total number of CPUs available on the current machine.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>>\n",
      "            >>> df = daft.from_pydict({\"foo\": [1, 2, 3], \"bar\": [\"a\", \"b\", \"c\"]})\n",
      "            >>> for row in df.iter_rows():\n",
      "            ...     print(row)\n",
      "            {'foo': 1, 'bar': 'a'}\n",
      "            {'foo': 2, 'bar': 'b'}\n",
      "            {'foo': 3, 'bar': 'c'}\n",
      "\n",
      "\n",
      "        Args:\n",
      "            results_buffer_size: how many partitions to allow in the results buffer (defaults to the total number of CPUs\n",
      "                available on the machine).\n",
      "            column_format: the format of the columns to iterate over. One of \"python\" or \"arrow\". Defaults to \"python\".\n",
      "\n",
      "        .. seealso::\n",
      "            :meth:`df.iter_partitions() <daft.DataFrame.iter_partitions>`: iterator over entire partitions instead of single rows\n",
      "        \"\"\"\n",
      "        if results_buffer_size == \"num_cpus\":\n",
      "            results_buffer_size = multiprocessing.cpu_count()\n",
      "\n",
      "        def arrow_iter_rows(table: \"pyarrow.Table\") -> Iterator[Dict[str, Any]]:\n",
      "            columns = table.columns\n",
      "            for i in range(len(table)):\n",
      "                row = {col._name: col[i] for col in columns}\n",
      "                yield row\n",
      "\n",
      "        def python_iter_rows(pydict: Dict[str, List[Any]], num_rows: int) -> Iterator[Dict[str, Any]]:\n",
      "            for i in range(num_rows):\n",
      "                row = {key: value[i] for (key, value) in pydict.items()}\n",
      "                yield row\n",
      "\n",
      "        if self._result is not None:\n",
      "            # If the dataframe has already finished executing,\n",
      "            # use the precomputed results.\n",
      "            if column_format == \"python\":\n",
      "                yield from python_iter_rows(self.to_pydict(), len(self))\n",
      "            elif column_format == \"arrow\":\n",
      "                yield from arrow_iter_rows(self.to_arrow())\n",
      "            else:\n",
      "                raise ValueError(\n",
      "                    f\"Unsupported column_format: {column_format}, supported formats are 'python' and 'arrow'\"\n",
      "                )\n",
      "        else:\n",
      "            # Execute the dataframe in a streaming fashion.\n",
      "            context = get_context()\n",
      "            partitions_iter = context.get_or_create_runner().run_iter_tables(\n",
      "                self._builder, results_buffer_size=results_buffer_size\n",
      "            )\n",
      "\n",
      "            # Iterate through partitions.\n",
      "            for partition in partitions_iter:\n",
      "                if column_format == \"python\":\n",
      "                    yield from python_iter_rows(partition.to_pydict(), len(partition))\n",
      "                elif column_format == \"arrow\":\n",
      "                    yield from arrow_iter_rows(partition.to_arrow())\n",
      "                else:\n",
      "                    raise ValueError(\n",
      "                        f\"Unsupported column_format: {column_format}, supported formats are 'python' and 'arrow'\"\n",
      "                    )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_arrow_iter(\n",
      "        self,\n",
      "        results_buffer_size: Union[Optional[int], Literal[\"num_cpus\"]] = \"num_cpus\",\n",
      "    ) -> Iterator[\"pyarrow.RecordBatch\"]:\n",
      "        \"\"\"Return an iterator of pyarrow recordbatches for this dataframe.\"\"\"\n",
      "        for name in self.schema().column_names():\n",
      "            if self.schema()[name].dtype._is_python_type():\n",
      "                raise ValueError(\n",
      "                    f\"Cannot convert column {name} to Arrow type, found Python type: {self.schema()[name].dtype}\"\n",
      "                )\n",
      "\n",
      "        if results_buffer_size == \"num_cpus\":\n",
      "            results_buffer_size = multiprocessing.cpu_count()\n",
      "        if results_buffer_size is not None and not results_buffer_size > 0:\n",
      "            raise ValueError(f\"Provided `results_buffer_size` value must be > 0, received: {results_buffer_size}\")\n",
      "        if self._result is not None:\n",
      "            # If the dataframe has already finished executing,\n",
      "            # use the precomputed results.\n",
      "            for _, result in self._result.items():\n",
      "                yield from (result.micropartition().to_arrow().to_batches())\n",
      "        else:\n",
      "            # Execute the dataframe in a streaming fashion.\n",
      "            context = get_context()\n",
      "            partitions_iter = context.get_or_create_runner().run_iter_tables(\n",
      "                self._builder, results_buffer_size=results_buffer_size\n",
      "            )\n",
      "\n",
      "            # Iterate through partitions.\n",
      "            for partition in partitions_iter:\n",
      "                yield from partition.to_arrow().to_batches()\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def iter_partitions(\n",
      "        self, results_buffer_size: Union[Optional[int], Literal[\"num_cpus\"]] = \"num_cpus\"\n",
      "    ) -> Iterator[Union[MicroPartition, \"ray.ObjectRef[MicroPartition]\"]]:\n",
      "        \"\"\"Begin executing this dataframe and return an iterator over the partitions.\n",
      "\n",
      "        Each partition will be returned as a daft.recordbatch object (if using Python runner backend)\n",
      "        or a ray ObjectRef (if using Ray runner backend).\n",
      "\n",
      "        .. NOTE::\n",
      "            A quick note on configuring asynchronous/parallel execution using `results_buffer_size`.\n",
      "\n",
      "            The `results_buffer_size` kwarg controls how many results Daft will allow to be in the buffer while iterating.\n",
      "            Once this buffer is filled, Daft will not run any more work until some partition is consumed from the buffer.\n",
      "\n",
      "            * Increasing this value means the iterator will consume more memory and CPU resources but have higher throughput\n",
      "            * Decreasing this value means the iterator will consume lower memory and CPU resources, but have lower throughput\n",
      "            * Setting this value to `None` means the iterator will consume as much resources as it deems appropriate per-iteration\n",
      "\n",
      "            The default value is the total number of CPUs available on the current machine.\n",
      "\n",
      "        Args:\n",
      "            results_buffer_size: how many partitions to allow in the results buffer (defaults to the total number of CPUs\n",
      "                available on the machine).\n",
      "\n",
      "        >>> import daft\n",
      "        >>>\n",
      "        >>> df = daft.from_pydict({\"foo\": [1, 2, 3], \"bar\": [\"a\", \"b\", \"c\"]}).into_partitions(2)\n",
      "        >>> for part in df.iter_partitions():\n",
      "        ...     print(part)\n",
      "        MicroPartition with 2 rows:\n",
      "        TableState: Loaded. 1 tables\n",
      "        ╭───────┬──────╮\n",
      "        │ foo   ┆ bar  │\n",
      "        │ ---   ┆ ---  │\n",
      "        │ Int64 ┆ Utf8 │\n",
      "        ╞═══════╪══════╡\n",
      "        │ 1     ┆ a    │\n",
      "        ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "        │ 2     ┆ b    │\n",
      "        ╰───────┴──────╯\n",
      "        <BLANKLINE>\n",
      "        <BLANKLINE>\n",
      "        Statistics: missing\n",
      "        <BLANKLINE>\n",
      "        MicroPartition with 1 rows:\n",
      "        TableState: Loaded. 1 tables\n",
      "        ╭───────┬──────╮\n",
      "        │ foo   ┆ bar  │\n",
      "        │ ---   ┆ ---  │\n",
      "        │ Int64 ┆ Utf8 │\n",
      "        ╞═══════╪══════╡\n",
      "        │ 3     ┆ c    │\n",
      "        ╰───────┴──────╯\n",
      "        <BLANKLINE>\n",
      "        <BLANKLINE>\n",
      "        Statistics: missing\n",
      "        <BLANKLINE>\n",
      "        \"\"\"\n",
      "        if results_buffer_size == \"num_cpus\":\n",
      "            results_buffer_size = multiprocessing.cpu_count()\n",
      "        elif results_buffer_size is not None and not results_buffer_size > 0:\n",
      "            raise ValueError(f\"Provided `results_buffer_size` value must be > 0, received: {results_buffer_size}\")\n",
      "\n",
      "        if self._result is not None:\n",
      "            # If the dataframe has already finished executing,\n",
      "            # use the precomputed results.\n",
      "            for mat_result in self._result.values():\n",
      "                yield mat_result.partition()\n",
      "\n",
      "        else:\n",
      "            # Execute the dataframe in a streaming fashion.\n",
      "            context = get_context()\n",
      "            results_iter = context.get_or_create_runner().run_iter(\n",
      "                self._builder, results_buffer_size=results_buffer_size\n",
      "            )\n",
      "            for result in results_iter:\n",
      "                yield result.partition()\n",
      "\n",
      "    def _populate_preview(self) -> None:\n",
      "        \"\"\"Populates the preview of the DataFrame, if it is not already populated.\"\"\"\n",
      "        if self._result is None:\n",
      "            return\n",
      "\n",
      "        preview_partition_invalid = (\n",
      "            self._preview.preview_partition is None or len(self._preview.preview_partition) < self._num_preview_rows\n",
      "        )\n",
      "        if preview_partition_invalid:\n",
      "            preview_parts = self._result._get_preview_micropartitions(self._num_preview_rows)\n",
      "            preview_results = LocalPartitionSet()\n",
      "            for i, part in enumerate(preview_parts):\n",
      "                preview_results.set_partition_from_table(i, part)\n",
      "            preview_partition = preview_results._get_merged_micropartition()\n",
      "            self._preview = DataFramePreview(\n",
      "                preview_partition=preview_partition,\n",
      "                dataframe_num_rows=len(self),\n",
      "            )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def __repr__(self) -> str:\n",
      "        self._populate_preview()\n",
      "        display = DataFrameDisplay(self._preview, self.schema())\n",
      "        return display.__repr__()\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def _repr_html_(self) -> str:\n",
      "        self._populate_preview()\n",
      "        display = DataFrameDisplay(self._preview, self.schema())\n",
      "        return display._repr_html_()\n",
      "\n",
      "    ###\n",
      "    # Creation methods\n",
      "    ###\n",
      "\n",
      "    @classmethod\n",
      "    def _from_pylist(cls, data: List[Dict[str, Any]]) -> \"DataFrame\":\n",
      "        \"\"\"Creates a DataFrame from a list of dictionaries.\"\"\"\n",
      "        headers: Set[str] = set()\n",
      "        for row in data:\n",
      "            if not isinstance(row, dict):\n",
      "                raise ValueError(f\"Expected list of dictionaries of {{column_name: value}}, received: {type(row)}\")\n",
      "            headers.update(row.keys())\n",
      "        headers_ordered = sorted(list(headers))\n",
      "        return cls._from_pydict(data={header: [row.get(header, None) for row in data] for header in headers_ordered})\n",
      "\n",
      "    @classmethod\n",
      "    def _from_pydict(cls, data: Dict[str, InputListType]) -> \"DataFrame\":\n",
      "        \"\"\"Creates a DataFrame from a Python dictionary.\"\"\"\n",
      "        column_lengths = {key: len(data[key]) for key in data}\n",
      "        if len(set(column_lengths.values())) > 1:\n",
      "            raise ValueError(\n",
      "                f\"Expected all columns to be of the same length, but received columns with lengths: {column_lengths}\"\n",
      "            )\n",
      "\n",
      "        data_micropartition = MicroPartition.from_pydict(data)\n",
      "        return cls._from_micropartitions(data_micropartition)\n",
      "\n",
      "    @classmethod\n",
      "    def _from_arrow(cls, data: Union[\"pyarrow.Table\", List[\"pyarrow.Table\"], Iterable[\"pyarrow.Table\"]]) -> \"DataFrame\":\n",
      "        \"\"\"Creates a DataFrame from a `pyarrow Table <https://arrow.apache.org/docs/python/generated/pyarrow.Table.html>`__.\"\"\"\n",
      "        if isinstance(data, Iterable):\n",
      "            data = list(data)\n",
      "        if not isinstance(data, list):\n",
      "            data = [data]\n",
      "        parts = [MicroPartition.from_arrow(table) for table in data]\n",
      "        return cls._from_micropartitions(*parts)\n",
      "\n",
      "    @classmethod\n",
      "    def _from_pandas(cls, data: Union[\"pandas.DataFrame\", List[\"pandas.DataFrame\"]]) -> \"DataFrame\":\n",
      "        \"\"\"Creates a Daft DataFrame from a `pandas DataFrame <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>`__.\"\"\"\n",
      "        if not isinstance(data, list):\n",
      "            data = [data]\n",
      "        parts = [MicroPartition.from_pandas(df) for df in data]\n",
      "        return cls._from_micropartitions(*parts)\n",
      "\n",
      "    @classmethod\n",
      "    def _from_micropartitions(cls, *parts: MicroPartition) -> \"DataFrame\":\n",
      "        \"\"\"Creates a Daft DataFrame from MicroPartition(s).\n",
      "\n",
      "        Args:\n",
      "            parts: The Tables that we wish to convert into a Daft DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Daft DataFrame created from the provided Table.\n",
      "        \"\"\"\n",
      "        if not parts:\n",
      "            raise ValueError(\"Can't create a DataFrame from an empty list of tables.\")\n",
      "\n",
      "        result_pset = LocalPartitionSet()\n",
      "\n",
      "        for i, part in enumerate(parts):\n",
      "            result_pset.set_partition_from_table(i, part)\n",
      "\n",
      "        context = get_context()\n",
      "        cache_entry = context.get_or_create_runner().put_partition_set_into_cache(result_pset)\n",
      "        size_bytes = result_pset.size_bytes()\n",
      "        num_rows = len(result_pset)\n",
      "\n",
      "        assert size_bytes is not None, \"In-memory data should always have non-None size in bytes\"\n",
      "        builder = LogicalPlanBuilder.from_in_memory_scan(\n",
      "            cache_entry, parts[0].schema(), result_pset.num_partitions(), size_bytes, num_rows=num_rows\n",
      "        )\n",
      "\n",
      "        df = cls(builder)\n",
      "        df._result_cache = cache_entry\n",
      "\n",
      "        # build preview\n",
      "        df._populate_preview()\n",
      "        return df\n",
      "\n",
      "    @classmethod\n",
      "    def _from_schema(cls, schema: Schema) -> \"DataFrame\":\n",
      "        \"\"\"Creates a Daft DataFrom from a Schema.\n",
      "\n",
      "        Args:\n",
      "            schema: The Schema to convert into a DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Daft DataFrame with \"column_name\" and \"type\" fields.\n",
      "        \"\"\"\n",
      "        pydict: Dict = {\"column_name\": [], \"type\": []}\n",
      "        for field in schema:\n",
      "            pydict[\"column_name\"].append(field.name)\n",
      "            pydict[\"type\"].append(str(field.dtype))\n",
      "        return DataFrame._from_pydict(pydict)\n",
      "\n",
      "    ###\n",
      "    # Write methods\n",
      "    ###\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def write_parquet(\n",
      "        self,\n",
      "        root_dir: Union[str, pathlib.Path],\n",
      "        compression: str = \"snappy\",\n",
      "        write_mode: Literal[\"append\", \"overwrite\", \"overwrite-partitions\"] = \"append\",\n",
      "        partition_cols: Optional[List[ColumnInputType]] = None,\n",
      "        io_config: Optional[IOConfig] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Writes the DataFrame as parquet files, returning a new DataFrame with paths to the files that were written.\n",
      "\n",
      "        Files will be written to ``<root_dir>/*`` with randomly generated UUIDs as the file names.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            root_dir (str): root file path to write parquet files to.\n",
      "            compression (str, optional): compression algorithm. Defaults to \"snappy\".\n",
      "            write_mode (str, optional): Operation mode of the write. `append` will add new data, `overwrite` will replace the contents of the root directory with new data. `overwrite-partitions` will replace only the contents in the partitions that are being written to. Defaults to \"append\".\n",
      "            partition_cols (Optional[List[ColumnInputType]], optional): How to subpartition each partition further. Defaults to None.\n",
      "            io_config (Optional[IOConfig], optional): configurations to use when interacting with remote storage.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The filenames that were written out as strings.\n",
      "\n",
      "            .. NOTE::\n",
      "                This call is **blocking** and will execute the DataFrame when called\n",
      "        \"\"\"\n",
      "        if write_mode not in [\"append\", \"overwrite\", \"overwrite-partitions\"]:\n",
      "            raise ValueError(\n",
      "                f\"Only support `append`, `overwrite`, or `overwrite-partitions` mode. {write_mode} is unsupported\"\n",
      "            )\n",
      "        if write_mode == \"overwrite-partitions\" and partition_cols is None:\n",
      "            raise ValueError(\"Partition columns must be specified to use `overwrite-partitions` mode.\")\n",
      "\n",
      "        io_config = get_context().daft_planning_config.default_io_config if io_config is None else io_config\n",
      "\n",
      "        cols: Optional[List[Expression]] = None\n",
      "        if partition_cols is not None:\n",
      "            cols = self.__column_input_to_expression(tuple(partition_cols))\n",
      "\n",
      "        builder = self._builder.write_tabular(\n",
      "            root_dir=root_dir,\n",
      "            partition_cols=cols,\n",
      "            file_format=FileFormat.Parquet,\n",
      "            compression=compression,\n",
      "            io_config=io_config,\n",
      "        )\n",
      "        # Block and write, then retrieve data\n",
      "        write_df = DataFrame(builder)\n",
      "        write_df.collect()\n",
      "        assert write_df._result is not None\n",
      "\n",
      "        if write_mode == \"overwrite\":\n",
      "            overwrite_files(write_df, root_dir, io_config, False)\n",
      "        elif write_mode == \"overwrite-partitions\":\n",
      "            overwrite_files(write_df, root_dir, io_config, True)\n",
      "\n",
      "        if len(write_df) > 0:\n",
      "            # Populate and return a new disconnected DataFrame\n",
      "            result_df = DataFrame(write_df._builder)\n",
      "            result_df._result_cache = write_df._result_cache\n",
      "            result_df._preview = write_df._preview\n",
      "            return result_df\n",
      "        else:\n",
      "            from daft import from_pydict\n",
      "            from daft.recordbatch.recordbatch_io import write_empty_tabular\n",
      "\n",
      "            file_path = write_empty_tabular(\n",
      "                root_dir, FileFormat.Parquet, self.schema(), compression=compression, io_config=io_config\n",
      "            )\n",
      "\n",
      "            return from_pydict(\n",
      "                {\n",
      "                    \"path\": [file_path],\n",
      "                }\n",
      "            )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def write_csv(\n",
      "        self,\n",
      "        root_dir: Union[str, pathlib.Path],\n",
      "        write_mode: Literal[\"append\", \"overwrite\", \"overwrite-partitions\"] = \"append\",\n",
      "        partition_cols: Optional[List[ColumnInputType]] = None,\n",
      "        io_config: Optional[IOConfig] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Writes the DataFrame as CSV files, returning a new DataFrame with paths to the files that were written.\n",
      "\n",
      "        Files will be written to ``<root_dir>/*`` with randomly generated UUIDs as the file names.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            root_dir (str): root file path to write parquet files to.\n",
      "            write_mode (str, optional): Operation mode of the write. `append` will add new data, `overwrite` will replace the contents of the root directory with new data. `overwrite-partitions` will replace only the contents in the partitions that are being written to. Defaults to \"append\".\n",
      "            partition_cols (Optional[List[ColumnInputType]], optional): How to subpartition each partition further. Defaults to None.\n",
      "            io_config (Optional[IOConfig], optional): configurations to use when interacting with remote storage.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The filenames that were written out as strings.\n",
      "        \"\"\"\n",
      "        if write_mode not in [\"append\", \"overwrite\", \"overwrite-partitions\"]:\n",
      "            raise ValueError(\n",
      "                f\"Only support `append`, `overwrite`, or `overwrite-partitions` mode. {write_mode} is unsupported\"\n",
      "            )\n",
      "        if write_mode == \"overwrite-partitions\" and partition_cols is None:\n",
      "            raise ValueError(\"Partition columns must be specified to use `overwrite-partitions` mode.\")\n",
      "\n",
      "        io_config = get_context().daft_planning_config.default_io_config if io_config is None else io_config\n",
      "\n",
      "        cols: Optional[List[Expression]] = None\n",
      "        if partition_cols is not None:\n",
      "            cols = self.__column_input_to_expression(tuple(partition_cols))\n",
      "        builder = self._builder.write_tabular(\n",
      "            root_dir=root_dir,\n",
      "            partition_cols=cols,\n",
      "            file_format=FileFormat.Csv,\n",
      "            io_config=io_config,\n",
      "        )\n",
      "\n",
      "        # Block and write, then retrieve data\n",
      "        write_df = DataFrame(builder)\n",
      "        write_df.collect()\n",
      "        assert write_df._result is not None\n",
      "\n",
      "        if write_mode == \"overwrite\":\n",
      "            overwrite_files(write_df, root_dir, io_config, False)\n",
      "        elif write_mode == \"overwrite-partitions\":\n",
      "            overwrite_files(write_df, root_dir, io_config, True)\n",
      "\n",
      "        if len(write_df) > 0:\n",
      "            # Populate and return a new disconnected DataFrame\n",
      "            result_df = DataFrame(write_df._builder)\n",
      "            result_df._result_cache = write_df._result_cache\n",
      "            result_df._preview = write_df._preview\n",
      "            return result_df\n",
      "        else:\n",
      "            from daft import from_pydict\n",
      "            from daft.recordbatch.recordbatch_io import write_empty_tabular\n",
      "\n",
      "            file_path = write_empty_tabular(root_dir, FileFormat.Csv, self.schema(), io_config=io_config)\n",
      "\n",
      "            return from_pydict(\n",
      "                {\n",
      "                    \"path\": [file_path],\n",
      "                }\n",
      "            )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def write_iceberg(\n",
      "        self, table: \"pyiceberg.table.Table\", mode: str = \"append\", io_config: Optional[IOConfig] = None\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Writes the DataFrame to an `Iceberg <https://iceberg.apache.org/docs/nightly/>`__ table, returning a new DataFrame with the operations that occurred.\n",
      "\n",
      "        Can be run in either `append` or `overwrite` mode which will either appends the rows in the DataFrame or will delete the existing rows and then append the DataFrame rows respectively.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            table (pyiceberg.table.Table): Destination `PyIceberg Table <https://py.iceberg.apache.org/reference/pyiceberg/table/#pyiceberg.table.Table>`__ to write dataframe to.\n",
      "            mode (str, optional): Operation mode of the write. `append` or `overwrite` Iceberg Table. Defaults to \"append\".\n",
      "            io_config (IOConfig, optional): A custom IOConfig to use when accessing Iceberg object storage data. If provided, configurations set in `table` are ignored.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The operations that occurred with this write.\n",
      "        \"\"\"\n",
      "        import pyarrow as pa\n",
      "        import pyiceberg\n",
      "        from packaging.version import parse\n",
      "\n",
      "        from daft.io._iceberg import _convert_iceberg_file_io_properties_to_io_config\n",
      "\n",
      "        if len(table.spec().fields) > 0 and parse(pyiceberg.__version__) < parse(\"0.7.0\"):\n",
      "            raise ValueError(\"pyiceberg>=0.7.0 is required to write to a partitioned table\")\n",
      "\n",
      "        if parse(pyiceberg.__version__) < parse(\"0.6.0\"):\n",
      "            raise ValueError(f\"Write Iceberg is only supported on pyiceberg>=0.6.0, found {pyiceberg.__version__}\")\n",
      "\n",
      "        if parse(pa.__version__) < parse(\"12.0.1\"):\n",
      "            raise ValueError(\n",
      "                f\"Write Iceberg is only supported on pyarrow>=12.0.1, found {pa.__version__}. See this issue for more information: https://github.com/apache/arrow/issues/37054#issuecomment-1668644887\"\n",
      "            )\n",
      "\n",
      "        if mode not in [\"append\", \"overwrite\"]:\n",
      "            raise ValueError(f\"Only support `append` or `overwrite` mode. {mode} is unsupported\")\n",
      "\n",
      "        io_config = (\n",
      "            _convert_iceberg_file_io_properties_to_io_config(table.io.properties) if io_config is None else io_config\n",
      "        )\n",
      "        io_config = get_context().daft_planning_config.default_io_config if io_config is None else io_config\n",
      "\n",
      "        operations = []\n",
      "        path = []\n",
      "        rows = []\n",
      "        size = []\n",
      "\n",
      "        builder = self._builder.write_iceberg(table, io_config)\n",
      "        write_df = DataFrame(builder)\n",
      "        write_df.collect()\n",
      "\n",
      "        write_result = write_df.to_pydict()\n",
      "        assert \"data_file\" in write_result\n",
      "        data_files = write_result[\"data_file\"]\n",
      "\n",
      "        if mode == \"overwrite\":\n",
      "            deleted_files = table.scan().plan_files()\n",
      "        else:\n",
      "            deleted_files = []\n",
      "\n",
      "        schema = table.schema()\n",
      "        partitioning: Dict[str, list] = {schema.find_field(field.source_id).name: [] for field in table.spec().fields}\n",
      "\n",
      "        for data_file in data_files:\n",
      "            operations.append(\"ADD\")\n",
      "            path.append(data_file.file_path)\n",
      "            rows.append(data_file.record_count)\n",
      "            size.append(data_file.file_size_in_bytes)\n",
      "\n",
      "            for field in partitioning.keys():\n",
      "                partitioning[field].append(getattr(data_file.partition, field, None))\n",
      "\n",
      "        for pf in deleted_files:\n",
      "            data_file = pf.file\n",
      "            operations.append(\"DELETE\")\n",
      "            path.append(data_file.file_path)\n",
      "            rows.append(data_file.record_count)\n",
      "            size.append(data_file.file_size_in_bytes)\n",
      "\n",
      "            for field in partitioning.keys():\n",
      "                partitioning[field].append(getattr(data_file.partition, field, None))\n",
      "\n",
      "        if parse(pyiceberg.__version__) >= parse(\"0.7.0\"):\n",
      "            from pyiceberg.table import ALWAYS_TRUE, TableProperties\n",
      "\n",
      "            if parse(pyiceberg.__version__) >= parse(\"0.8.0\"):\n",
      "                from pyiceberg.utils.properties import property_as_bool\n",
      "\n",
      "                property_as_bool = property_as_bool\n",
      "            else:\n",
      "                from pyiceberg.table import PropertyUtil\n",
      "\n",
      "                property_as_bool = PropertyUtil.property_as_bool\n",
      "\n",
      "            tx = table.transaction()\n",
      "\n",
      "            if mode == \"overwrite\":\n",
      "                tx.delete(delete_filter=ALWAYS_TRUE)\n",
      "\n",
      "            update_snapshot = tx.update_snapshot()\n",
      "\n",
      "            manifest_merge_enabled = mode == \"append\" and property_as_bool(\n",
      "                tx.table_metadata.properties,\n",
      "                TableProperties.MANIFEST_MERGE_ENABLED,\n",
      "                TableProperties.MANIFEST_MERGE_ENABLED_DEFAULT,\n",
      "            )\n",
      "\n",
      "            append_method = update_snapshot.merge_append if manifest_merge_enabled else update_snapshot.fast_append\n",
      "\n",
      "            with append_method() as append_files:\n",
      "                for data_file in data_files:\n",
      "                    append_files.append_data_file(data_file)\n",
      "\n",
      "            tx.commit_transaction()\n",
      "        else:\n",
      "            from pyiceberg.table import _MergingSnapshotProducer\n",
      "            from pyiceberg.table.snapshots import Operation\n",
      "\n",
      "            operations_map = {\n",
      "                \"append\": Operation.APPEND,\n",
      "                \"overwrite\": Operation.OVERWRITE,\n",
      "            }\n",
      "\n",
      "            merge = _MergingSnapshotProducer(operation=operations_map[mode], table=table)\n",
      "\n",
      "            for data_file in data_files:\n",
      "                merge.append_data_file(data_file)\n",
      "\n",
      "            merge.commit()\n",
      "\n",
      "        with_operations = {\n",
      "            \"operation\": pa.array(operations, type=pa.string()),\n",
      "            \"rows\": pa.array(rows, type=pa.int64()),\n",
      "            \"file_size\": pa.array(size, type=pa.int64()),\n",
      "            \"file_name\": pa.array([fp for fp in path], type=pa.string()),\n",
      "        }\n",
      "\n",
      "        if partitioning:\n",
      "            with_operations[\"partitioning\"] = pa.StructArray.from_arrays(\n",
      "                partitioning.values(), names=partitioning.keys()\n",
      "            )\n",
      "\n",
      "        from daft import from_pydict\n",
      "\n",
      "        # NOTE: We are losing the history of the plan here.\n",
      "        # This is due to the fact that the logical plan of the write_iceberg returns datafiles but we want to return the above data\n",
      "        return from_pydict(with_operations)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def write_deltalake(\n",
      "        self,\n",
      "        table: Union[str, pathlib.Path, \"DataCatalogTable\", \"deltalake.DeltaTable\", \"UnityCatalogTable\"],\n",
      "        partition_cols: Optional[List[str]] = None,\n",
      "        mode: Literal[\"append\", \"overwrite\", \"error\", \"ignore\"] = \"append\",\n",
      "        schema_mode: Optional[Literal[\"merge\", \"overwrite\"]] = None,\n",
      "        name: Optional[str] = None,\n",
      "        description: Optional[str] = None,\n",
      "        configuration: Optional[Mapping[str, Optional[str]]] = None,\n",
      "        custom_metadata: Optional[Dict[str, str]] = None,\n",
      "        dynamo_table_name: Optional[str] = None,\n",
      "        allow_unsafe_rename: bool = False,\n",
      "        io_config: Optional[IOConfig] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Writes the DataFrame to a `Delta Lake <https://docs.delta.io/latest/index.html>`__ table, returning a new DataFrame with the operations that occurred.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            table (Union[str, pathlib.Path, DataCatalogTable, deltalake.DeltaTable, UnityCatalogTable]): Destination `Delta Lake Table <https://delta-io.github.io/delta-rs/api/delta_table/>`__ or table URI to write dataframe to.\n",
      "            partition_cols (List[str], optional): How to subpartition each partition further. If table exists, expected to match table's existing partitioning scheme, otherwise creates the table with specified partition columns. Defaults to None.\n",
      "            mode (str, optional): Operation mode of the write. `append` will add new data, `overwrite` will replace table with new data, `error` will raise an error if table already exists, and `ignore` will not write anything if table already exists. Defaults to \"append\".\n",
      "            schema_mode (str, optional): Schema mode of the write. If set to `overwrite`, allows replacing the schema of the table when doing `mode=overwrite`. Schema mode `merge` is currently not supported.\n",
      "            name (str, optional): User-provided identifier for this table.\n",
      "            description (str, optional): User-provided description for this table.\n",
      "            configuration (Mapping[str, Optional[str]], optional): A map containing configuration options for the metadata action.\n",
      "            custom_metadata (Dict[str, str], optional): Custom metadata to add to the commit info.\n",
      "            dynamo_table_name (str, optional): Name of the DynamoDB table to be used as the locking provider if writing to S3.\n",
      "            allow_unsafe_rename (bool, optional): Whether to allow unsafe rename when writing to S3 or local disk. Defaults to False.\n",
      "            io_config (IOConfig, optional): configurations to use when interacting with remote storage.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The operations that occurred with this write.\n",
      "        \"\"\"\n",
      "        import json\n",
      "\n",
      "        import deltalake\n",
      "        import pyarrow as pa\n",
      "        from deltalake.schema import _convert_pa_schema_to_delta\n",
      "        from deltalake.writer import AddAction, try_get_deltatable, write_deltalake_pyarrow\n",
      "        from packaging.version import parse\n",
      "\n",
      "        from daft import from_pydict\n",
      "        from daft.dependencies import unity_catalog\n",
      "        from daft.filesystem import get_protocol_from_path\n",
      "        from daft.io import DataCatalogTable\n",
      "        from daft.io._deltalake import large_dtypes_kwargs\n",
      "        from daft.io.object_store_options import io_config_to_storage_options\n",
      "\n",
      "        def _create_metadata_param(metadata: Optional[Dict[str, str]]):\n",
      "            \"\"\"From deltalake>=0.20.0 onwards, custom_metadata has to be passed as CommitProperties.\n",
      "\n",
      "            Args:\n",
      "                metadata\n",
      "\n",
      "            Returns:\n",
      "                DataFrame: metadata for deltalake<0.20.0, otherwise CommitProperties with custom_metadata\n",
      "            \"\"\"\n",
      "            if parse(deltalake.__version__) < parse(\"0.20.0\"):\n",
      "                return metadata\n",
      "            else:\n",
      "                from deltalake import CommitProperties\n",
      "\n",
      "                return CommitProperties(custom_metadata=metadata)\n",
      "\n",
      "        if schema_mode == \"merge\":\n",
      "            raise ValueError(\"Schema mode' merge' is not currently supported for write_deltalake.\")\n",
      "\n",
      "        if parse(deltalake.__version__) < parse(\"0.14.0\"):\n",
      "            raise ValueError(f\"Write delta lake is only supported on deltalake>=0.14.0, found {deltalake.__version__}\")\n",
      "\n",
      "        io_config = get_context().daft_planning_config.default_io_config if io_config is None else io_config\n",
      "\n",
      "        # Retrieve table_uri and storage_options from various backends\n",
      "        table_uri: str\n",
      "        storage_options: dict\n",
      "\n",
      "        if isinstance(table, deltalake.DeltaTable):\n",
      "            table_uri = table.table_uri\n",
      "            storage_options = table._storage_options or {}\n",
      "            new_storage_options = io_config_to_storage_options(io_config, table_uri)\n",
      "            storage_options.update(new_storage_options or {})\n",
      "        else:\n",
      "            if isinstance(table, str):\n",
      "                table_uri = table\n",
      "            elif isinstance(table, pathlib.Path):\n",
      "                table_uri = str(table)\n",
      "            elif unity_catalog.module_available() and isinstance(table, unity_catalog.UnityCatalogTable):\n",
      "                table_uri = table.table_uri\n",
      "                io_config = table.io_config\n",
      "            elif isinstance(table, DataCatalogTable):\n",
      "                table_uri = table.table_uri(io_config)\n",
      "            else:\n",
      "                raise ValueError(f\"Expected table to be a path or a DeltaTable, received: {type(table)}\")\n",
      "\n",
      "            if io_config is None:\n",
      "                raise ValueError(\n",
      "                    \"io_config was not provided to write_deltalake and could not be retrieved from defaults.\"\n",
      "                )\n",
      "\n",
      "            storage_options = io_config_to_storage_options(io_config, table_uri) or {}\n",
      "            table = try_get_deltatable(table_uri, storage_options=storage_options)\n",
      "\n",
      "        # see: https://delta-io.github.io/delta-rs/usage/writing/writing-to-s3-with-locking-provider/\n",
      "        scheme = get_protocol_from_path(table_uri)\n",
      "        if scheme == \"s3\" or scheme == \"s3a\":\n",
      "            if dynamo_table_name is not None:\n",
      "                storage_options[\"AWS_S3_LOCKING_PROVIDER\"] = \"dynamodb\"\n",
      "                storage_options[\"DELTA_DYNAMO_TABLE_NAME\"] = dynamo_table_name\n",
      "            else:\n",
      "                storage_options[\"AWS_S3_ALLOW_UNSAFE_RENAME\"] = \"true\"\n",
      "\n",
      "                if not allow_unsafe_rename:\n",
      "                    warnings.warn(\"No DynamoDB table specified for Delta Lake locking. Defaulting to unsafe writes.\")\n",
      "        elif scheme == \"file\":\n",
      "            if allow_unsafe_rename:\n",
      "                storage_options[\"MOUNT_ALLOW_UNSAFE_RENAME\"] = \"true\"\n",
      "\n",
      "        pyarrow_schema = pa.schema((f.name, f.dtype.to_arrow_dtype()) for f in self.schema())\n",
      "\n",
      "        large_dtypes = True\n",
      "        delta_schema = _convert_pa_schema_to_delta(pyarrow_schema, **large_dtypes_kwargs(large_dtypes))\n",
      "\n",
      "        if table:\n",
      "            if partition_cols and partition_cols != table.metadata().partition_columns:\n",
      "                raise ValueError(\n",
      "                    f\"Expected partition columns to match that of the existing table ({table.metadata().partition_columns}), but received: {partition_cols}\"\n",
      "                )\n",
      "            else:\n",
      "                partition_cols = table.metadata().partition_columns\n",
      "\n",
      "            table.update_incremental()\n",
      "\n",
      "            table_schema = table.schema().to_pyarrow(as_large_types=large_dtypes)\n",
      "            if delta_schema != table_schema and not (mode == \"overwrite\" and schema_mode == \"overwrite\"):\n",
      "                raise ValueError(\n",
      "                    \"Schema of data does not match table schema\\n\"\n",
      "                    f\"Data schema:\\n{delta_schema}\\nTable Schema:\\n{table_schema}\"\n",
      "                )\n",
      "            if mode == \"error\":\n",
      "                raise AssertionError(\"Delta table already exists, write mode set to error.\")\n",
      "            elif mode == \"ignore\":\n",
      "                return from_pydict(\n",
      "                    {\n",
      "                        \"operation\": pa.array([], type=pa.string()),\n",
      "                        \"rows\": pa.array([], type=pa.int64()),\n",
      "                        \"file_size\": pa.array([], type=pa.int64()),\n",
      "                        \"file_name\": pa.array([], type=pa.string()),\n",
      "                    }\n",
      "                )\n",
      "            version = table.version() + 1\n",
      "        else:\n",
      "            version = 0\n",
      "\n",
      "        if partition_cols is not None:\n",
      "            for c in partition_cols:\n",
      "                if self.schema()[c].dtype == DataType.binary():\n",
      "                    raise NotImplementedError(\"Binary partition columns are not yet supported for Delta Lake writes\")\n",
      "\n",
      "        builder = self._builder.write_deltalake(\n",
      "            table_uri,\n",
      "            mode,\n",
      "            version,\n",
      "            large_dtypes,\n",
      "            io_config=io_config,\n",
      "            partition_cols=partition_cols,\n",
      "        )\n",
      "        write_df = DataFrame(builder)\n",
      "        write_df.collect()\n",
      "\n",
      "        write_result = write_df.to_pydict()\n",
      "        assert \"add_action\" in write_result\n",
      "        add_actions: List[AddAction] = write_result[\"add_action\"]\n",
      "\n",
      "        operations = []\n",
      "        paths = []\n",
      "        rows = []\n",
      "        sizes = []\n",
      "\n",
      "        for add_action in add_actions:\n",
      "            stats = json.loads(add_action.stats)\n",
      "            operations.append(\"ADD\")\n",
      "            paths.append(add_action.path)\n",
      "            rows.append(stats[\"numRecords\"])\n",
      "            sizes.append(add_action.size)\n",
      "\n",
      "        if table is None:\n",
      "            write_deltalake_pyarrow(\n",
      "                table_uri,\n",
      "                delta_schema,\n",
      "                add_actions,\n",
      "                mode,\n",
      "                partition_cols or [],\n",
      "                name,\n",
      "                description,\n",
      "                configuration,\n",
      "                storage_options,\n",
      "                custom_metadata,\n",
      "            )\n",
      "        else:\n",
      "            if mode == \"overwrite\":\n",
      "                old_actions = table.get_add_actions()\n",
      "                old_actions_dict = old_actions.to_pydict()\n",
      "                for i in range(old_actions.num_rows):\n",
      "                    operations.append(\"DELETE\")\n",
      "                    paths.append(old_actions_dict[\"path\"][i])\n",
      "                    rows.append(old_actions_dict[\"num_records\"][i])\n",
      "                    sizes.append(old_actions_dict[\"size_bytes\"][i])\n",
      "\n",
      "            metadata_param = _create_metadata_param(custom_metadata)\n",
      "            table._table.create_write_transaction(\n",
      "                add_actions, mode, partition_cols or [], delta_schema, None, metadata_param\n",
      "            )\n",
      "            table.update_incremental()\n",
      "\n",
      "        with_operations = from_pydict(\n",
      "            {\n",
      "                \"operation\": pa.array(operations, type=pa.string()),\n",
      "                \"rows\": pa.array(rows, type=pa.int64()),\n",
      "                \"file_size\": pa.array(sizes, type=pa.int64()),\n",
      "                \"file_name\": pa.array([os.path.basename(fp) for fp in paths], type=pa.string()),\n",
      "            }\n",
      "        )\n",
      "\n",
      "        return with_operations\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def write_lance(\n",
      "        self,\n",
      "        uri: Union[str, pathlib.Path],\n",
      "        mode: Literal[\"create\", \"append\", \"overwrite\"] = \"create\",\n",
      "        io_config: Optional[IOConfig] = None,\n",
      "        **kwargs,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Writes the DataFrame to a Lance table.\n",
      "\n",
      "        Note:\n",
      "            `write_lance` requires python 3.9 or higher\n",
      "        Args:\n",
      "          uri: The URI of the Lance table to write to\n",
      "          mode: The write mode. One of \"create\", \"append\", or \"overwrite\"\n",
      "          io_config (IOConfig, optional): configurations to use when interacting with remote storage.\n",
      "          **kwargs: Additional keyword arguments to pass to the Lance writer.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1, 2, 3, 4]})\n",
      "            >>> df.write_lance(\"/tmp/lance/my_table.lance\")  # doctest: +SKIP\n",
      "            ╭───────────────┬──────────────────┬─────────────────┬─────────╮\n",
      "            │ num_fragments ┆ num_deleted_rows ┆ num_small_files ┆ version │\n",
      "            │ ---           ┆ ---              ┆ ---             ┆ ---     │\n",
      "            │ Int64         ┆ Int64            ┆ Int64           ┆ Int64   │\n",
      "            ╞═══════════════╪══════════════════╪═════════════════╪═════════╡\n",
      "            │ 1             ┆ 0                ┆ 1               ┆ 1       │\n",
      "            ╰───────────────┴──────────────────┴─────────────────┴─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "            >>> daft.read_lance(\"/tmp/lance/my_table.lance\").collect()  # doctest: +SKIP\n",
      "            ╭───────╮\n",
      "            │ a     │\n",
      "            │ ---   │\n",
      "            │ Int64 │\n",
      "            ╞═══════╡\n",
      "            │ 1     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 2     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 3     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 4     │\n",
      "            ╰───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 4 of 4 rows)\n",
      "            >>> # Pass additional keyword arguments to the Lance writer\n",
      "            >>> # All additional keyword arguments are passed to `lance.write_fragments`\n",
      "            >>> df.write_lance(\"/tmp/lance/my_table.lance\", mode=\"overwrite\", max_bytes_per_file=1024)  # doctest: +SKIP\n",
      "            ╭───────────────┬──────────────────┬─────────────────┬─────────╮\n",
      "            │ num_fragments ┆ num_deleted_rows ┆ num_small_files ┆ version │\n",
      "            │ ---           ┆ ---              ┆ ---             ┆ ---     │\n",
      "            │ Int64         ┆ Int64            ┆ Int64           ┆ Int64   │\n",
      "            ╞═══════════════╪══════════════════╪═════════════════╪═════════╡\n",
      "            │ 1             ┆ 0                ┆ 1               ┆ 2       │\n",
      "            ╰───────────────┴──────────────────┴─────────────────┴─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "        \"\"\"\n",
      "        from daft import from_pydict\n",
      "        from daft.io.object_store_options import io_config_to_storage_options\n",
      "\n",
      "        try:\n",
      "            import lance\n",
      "            import pyarrow as pa\n",
      "\n",
      "        except ImportError:\n",
      "            raise ImportError(\"lance is not installed. Please install lance using `pip install daft[lance]`\")\n",
      "\n",
      "        io_config = get_context().daft_planning_config.default_io_config if io_config is None else io_config\n",
      "\n",
      "        if isinstance(uri, (str, pathlib.Path)):\n",
      "            if isinstance(uri, str):\n",
      "                table_uri = uri\n",
      "            elif isinstance(uri, pathlib.Path):\n",
      "                table_uri = str(uri)\n",
      "            else:\n",
      "                table_uri = uri\n",
      "        pyarrow_schema = pa.schema((f.name, f.dtype.to_arrow_dtype()) for f in self.schema())\n",
      "\n",
      "        storage_options = io_config_to_storage_options(io_config, table_uri)\n",
      "\n",
      "        try:\n",
      "            table = lance.dataset(table_uri, storage_options=storage_options)\n",
      "\n",
      "        except ValueError:\n",
      "            table = None\n",
      "\n",
      "        version = 0\n",
      "        if table:\n",
      "            table_schema = table.schema\n",
      "            version = table.latest_version\n",
      "            if pyarrow_schema != table_schema and not (mode == \"overwrite\"):\n",
      "                raise ValueError(\n",
      "                    \"Schema of data does not match table schema\\n\"\n",
      "                    f\"Data schema:\\n{pyarrow_schema}\\nTable Schema:\\n{table_schema}\"\n",
      "                )\n",
      "\n",
      "        builder = self._builder.write_lance(\n",
      "            table_uri,\n",
      "            mode,\n",
      "            io_config=io_config,\n",
      "            kwargs=kwargs,\n",
      "        )\n",
      "        write_df = DataFrame(builder)\n",
      "        write_df.collect()\n",
      "\n",
      "        write_result = write_df.to_pydict()\n",
      "        assert \"fragments\" in write_result\n",
      "        fragments = write_result[\"fragments\"]\n",
      "\n",
      "        if mode == \"create\" or mode == \"overwrite\":\n",
      "            operation = lance.LanceOperation.Overwrite(pyarrow_schema, fragments)\n",
      "        elif mode == \"append\":\n",
      "            operation = lance.LanceOperation.Append(fragments)\n",
      "\n",
      "        dataset = lance.LanceDataset.commit(table_uri, operation, read_version=version, storage_options=storage_options)\n",
      "        stats = dataset.stats.dataset_stats()\n",
      "\n",
      "        tbl = from_pydict(\n",
      "            {\n",
      "                \"num_fragments\": pa.array([stats[\"num_fragments\"]], type=pa.int64()),\n",
      "                \"num_deleted_rows\": pa.array([stats[\"num_deleted_rows\"]], type=pa.int64()),\n",
      "                \"num_small_files\": pa.array([stats[\"num_small_files\"]], type=pa.int64()),\n",
      "                \"version\": pa.array([dataset.version], type=pa.int64()),\n",
      "            }\n",
      "        )\n",
      "        return tbl\n",
      "\n",
      "    ###\n",
      "    # DataFrame operations\n",
      "    ###\n",
      "\n",
      "    def __column_input_to_expression(self, columns: Iterable[ColumnInputType]) -> List[Expression]:\n",
      "        # TODO(Kevin): remove this method and use _column_inputs_to_expressions\n",
      "        return [col(c) if isinstance(c, str) else c for c in columns]\n",
      "\n",
      "    def _is_column_input(self, x: Any) -> bool:\n",
      "        return isinstance(x, str) or isinstance(x, Expression)\n",
      "\n",
      "    def _column_inputs_to_expressions(self, columns: ManyColumnsInputType) -> List[Expression]:\n",
      "        \"\"\"Inputs to dataframe operations can be passed in as individual arguments or an iterable.\n",
      "\n",
      "        In addition, they may be strings or Expressions.\n",
      "        This method normalizes the inputs to a list of Expressions.\n",
      "        \"\"\"\n",
      "        column_iter: Iterable[ColumnInputType] = [columns] if self._is_column_input(columns) else columns  # type: ignore\n",
      "        return [col(c) if isinstance(c, str) else c for c in column_iter]\n",
      "\n",
      "    def _wildcard_inputs_to_expressions(self, columns: Tuple[ManyColumnsInputType, ...]) -> List[Expression]:\n",
      "        \"\"\"Handles wildcard argument column inputs.\"\"\"\n",
      "        column_input: Iterable[ColumnInputType] = columns[0] if len(columns) == 1 else columns  # type: ignore\n",
      "        return self._column_inputs_to_expressions(column_input)\n",
      "\n",
      "    def __getitem__(self, item: Union[slice, int, str, Iterable[Union[str, int]]]) -> Union[Expression, \"DataFrame\"]:\n",
      "        \"\"\"Gets a column from the DataFrame as an Expression (``df[\"mycol\"]``).\"\"\"\n",
      "        result: Optional[Expression]\n",
      "\n",
      "        if isinstance(item, int):\n",
      "            schema = self._builder.schema()\n",
      "            if item < -len(schema) or item >= len(schema):\n",
      "                raise ValueError(f\"{item} out of bounds for {schema}\")\n",
      "            result = ExpressionsProjection.from_schema(schema)[item]\n",
      "            assert result is not None\n",
      "            return result\n",
      "        elif isinstance(item, str):\n",
      "            schema = self._builder.schema()\n",
      "            if item not in schema.column_names() and item != \"*\":\n",
      "                raise ValueError(f\"{item} does not exist in schema {schema}\")\n",
      "\n",
      "            return col(item)\n",
      "        elif isinstance(item, Iterable):\n",
      "            schema = self._builder.schema()\n",
      "\n",
      "            columns = []\n",
      "            for it in item:\n",
      "                if isinstance(it, str):\n",
      "                    result = col(schema[it].name)\n",
      "                    columns.append(result)\n",
      "                elif isinstance(it, int):\n",
      "                    if it < -len(schema) or it >= len(schema):\n",
      "                        raise ValueError(f\"{it} out of bounds for {schema}\")\n",
      "                    field = list(self._builder.schema())[it]\n",
      "                    columns.append(col(field.name))\n",
      "                else:\n",
      "                    raise ValueError(f\"unknown indexing type: {type(it)}\")\n",
      "            return self.select(*columns)\n",
      "        elif isinstance(item, slice):\n",
      "            schema = self._builder.schema()\n",
      "            columns_exprs: ExpressionsProjection = ExpressionsProjection.from_schema(schema)\n",
      "            selected_columns = columns_exprs[item]\n",
      "            return self.select(*selected_columns)\n",
      "        else:\n",
      "            raise ValueError(f\"unknown indexing type: {type(item)}\")\n",
      "\n",
      "    def _add_monotonically_increasing_id(self, column_name: Optional[str] = None) -> \"DataFrame\":\n",
      "        \"\"\"Generates a column of monotonically increasing unique ids for the DataFrame.\n",
      "\n",
      "        The implementation of this method puts the partition number in the upper 28 bits, and the row number in each partition\n",
      "        in the lower 36 bits. This allows for 2^28 ≈ 268 million partitions and 2^40 ≈ 68 billion rows per partition.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1, 2, 3, 4]}).into_partitions(2)\n",
      "            >>> df = df._add_monotonically_increasing_id()\n",
      "            >>> df.show()\n",
      "            ╭─────────────┬───────╮\n",
      "            │ id          ┆ a     │\n",
      "            │ ---         ┆ ---   │\n",
      "            │ UInt64      ┆ Int64 │\n",
      "            ╞═════════════╪═══════╡\n",
      "            │ 0           ┆ 1     │\n",
      "            ├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1           ┆ 2     │\n",
      "            ├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 68719476736 ┆ 3     │\n",
      "            ├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 68719476737 ┆ 4     │\n",
      "            ╰─────────────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 4 of 4 rows)\n",
      "\n",
      "        Args:\n",
      "            column_name (Optional[str], optional): name of the new column. Defaults to \"id\".\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with a new column of monotonically increasing ids.\n",
      "        \"\"\"\n",
      "        builder = self._builder.add_monotonically_increasing_id(column_name)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def select(self, *columns: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Creates a new DataFrame from the provided expressions, similar to a SQL ``SELECT``.\n",
      "\n",
      "        Examples:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> df = df.select(\"x\", daft.col(\"y\"), daft.col(\"z\") + 1)\n",
      "            >>> df.show()\n",
      "            ╭───────┬───────┬───────╮\n",
      "            │ x     ┆ y     ┆ z     │\n",
      "            │ ---   ┆ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╪═══════╡\n",
      "            │ 1     ┆ 4     ┆ 8     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     ┆ 9     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     ┆ 10    │\n",
      "            ╰───────┴───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            *columns (Union[str, Expression]): columns to select from the current DataFrame\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: new DataFrame that will select the passed in columns\n",
      "        \"\"\"\n",
      "        assert len(columns) > 0\n",
      "        builder = self._builder.select(self.__column_input_to_expression(columns))\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def describe(self) -> \"DataFrame\":\n",
      "        \"\"\"Returns the Schema of the DataFrame, which provides information about each column, as a new DataFrame.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1, 2, 3], \"b\": [\"x\", \"y\", \"z\"]})\n",
      "            >>> df.describe().show()\n",
      "            ╭─────────────┬───────╮\n",
      "            │ column_name ┆ type  │\n",
      "            │ ---         ┆ ---   │\n",
      "            │ Utf8        ┆ Utf8  │\n",
      "            ╞═════════════╪═══════╡\n",
      "            │ a           ┆ Int64 │\n",
      "            ├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ b           ┆ Utf8  │\n",
      "            ╰─────────────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: A dataframe where each row is a column name and its corresponding type.\n",
      "        \"\"\"\n",
      "        builder = self.__builder.describe()\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def summarize(self) -> \"DataFrame\":\n",
      "        \"\"\"Returns column statistics for the DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: new DataFrame with the computed column statistics.\n",
      "        \"\"\"\n",
      "        builder = self._builder.summarize()\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def distinct(self) -> \"DataFrame\":\n",
      "        \"\"\"Computes unique rows, dropping duplicates.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 2], \"y\": [4, 5, 5], \"z\": [7, 8, 8]})\n",
      "            >>> unique_df = df.distinct()\n",
      "            >>> unique_df.show()\n",
      "            ╭───────┬───────┬───────╮\n",
      "            │ x     ┆ y     ┆ z     │\n",
      "            │ ---   ┆ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╪═══════╡\n",
      "            │ 2     ┆ 5     ┆ 8     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1     ┆ 4     ┆ 7     │\n",
      "            ╰───────┴───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame that has only  unique rows.\n",
      "        \"\"\"\n",
      "        ExpressionsProjection.from_schema(self._builder.schema())\n",
      "        builder = self._builder.distinct()\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def sample(\n",
      "        self,\n",
      "        fraction: float,\n",
      "        with_replacement: bool = False,\n",
      "        seed: Optional[int] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Samples a fraction of rows from the DataFrame.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> sampled_df = df.sample(0.5)\n",
      "            >>> # Samples will vary from output to output\n",
      "            >>> # here is a sample output\n",
      "            >>> # ╭───────┬───────┬───────╮\n",
      "            >>> # │ x     ┆ y     ┆ z     │\n",
      "            >>> # │ ---   ┆ ---   ┆ ---   │\n",
      "            >>> # │ Int64 ┆ Int64 ┆ Int64 │\n",
      "            >>> # |═══════╪═══════╪═══════╡\n",
      "            >>> # │ 2     ┆ 5     ┆ 8     │\n",
      "            >>> # ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            >>> # │ 3     ┆ 6     ┆ 9     │\n",
      "            >>> # ╰───────┴───────┴───────╯\n",
      "\n",
      "        Args:\n",
      "            fraction (float): fraction of rows to sample.\n",
      "            with_replacement (bool, optional): whether to sample with replacement. Defaults to False.\n",
      "            seed (Optional[int], optional): random seed. Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with a fraction of rows.\n",
      "        \"\"\"\n",
      "        if fraction < 0.0 or fraction > 1.0:\n",
      "            raise ValueError(f\"fraction should be between 0.0 and 1.0, but got {fraction}\")\n",
      "\n",
      "        builder = self._builder.sample(fraction, with_replacement, seed)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def exclude(self, *names: str) -> \"DataFrame\":\n",
      "        \"\"\"Drops columns from the current DataFrame by name.\n",
      "\n",
      "        This is equivalent of performing a select with all the columns but the ones excluded.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> df_without_x = df.exclude(\"x\")\n",
      "            >>> df_without_x.show()\n",
      "            ╭───────┬───────╮\n",
      "            │ y     ┆ z     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 4     ┆ 7     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 5     ┆ 8     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 6     ┆ 9     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            *names (str): names to exclude\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with some columns excluded.\n",
      "        \"\"\"\n",
      "        builder = self._builder.exclude(list(names))\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def filter(self, predicate: Union[Expression, str]) -> \"DataFrame\":\n",
      "        \"\"\"Filters rows via a predicate expression, similar to SQL ``WHERE``.\n",
      "\n",
      "        Alias for daft.DataFrame.where.\n",
      "\n",
      "        .. seealso::\n",
      "            :meth:`.where(predicate) <DataFrame.where>`\n",
      "\n",
      "        Args:\n",
      "            predicate (Expression): expression that keeps row if evaluates to True.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Filtered DataFrame.\n",
      "        \"\"\"\n",
      "        return self.where(predicate)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def where(self, predicate: Union[Expression, str]) -> \"DataFrame\":\n",
      "        \"\"\"Filters rows via a predicate expression, similar to SQL ``WHERE``.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 6, 6], \"z\": [7, 8, 9]})\n",
      "            >>> df.where((col(\"x\") > 1) & (col(\"y\") > 1)).collect()\n",
      "            ╭───────┬───────┬───────╮\n",
      "            │ x     ┆ y     ┆ z     │\n",
      "            │ ---   ┆ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╪═══════╡\n",
      "            │ 2     ┆ 6     ┆ 8     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     ┆ 9     │\n",
      "            ╰───────┴───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "            You can also use a string expression as a predicate.\n",
      "\n",
      "            Note: this will use the method `sql_expr` to parse the string into an expression\n",
      "            this may raise an error if the expression is not yet supported in the sql engine.\n",
      "\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 9, 9]})\n",
      "            >>> df.where(\"z = 9 AND y > 5\").collect()\n",
      "            ╭───────┬───────┬───────╮\n",
      "            │ x     ┆ y     ┆ z     │\n",
      "            │ ---   ┆ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╪═══════╡\n",
      "            │ 3     ┆ 6     ┆ 9     │\n",
      "            ╰───────┴───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        Args:\n",
      "            predicate (Expression): expression that keeps row if evaluates to True.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Filtered DataFrame.\n",
      "        \"\"\"\n",
      "        if isinstance(predicate, str):\n",
      "            from daft.sql.sql import sql_expr\n",
      "\n",
      "            predicate = sql_expr(predicate)\n",
      "        builder = self._builder.filter(predicate)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def with_column(\n",
      "        self,\n",
      "        column_name: str,\n",
      "        expr: Expression,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Adds a column to the current DataFrame with an Expression, equivalent to a ``select`` with all current columns and the new one.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3]})\n",
      "            >>> new_df = df.with_column(\"x+1\", col(\"x\") + 1)\n",
      "            >>> new_df.show()\n",
      "            ╭───────┬───────╮\n",
      "            │ x     ┆ x+1   │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 2     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 3     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 4     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            column_name (str): name of new column\n",
      "            expr (Expression): expression of the new column.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with new column.\n",
      "        \"\"\"\n",
      "        return self.with_columns({column_name: expr})\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def with_columns(\n",
      "        self,\n",
      "        columns: Dict[str, Expression],\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Adds columns to the current DataFrame with Expressions, equivalent to a ``select`` with all current columns and the new ones.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n",
      "            >>> new_df = df.with_columns({\"foo\": df[\"x\"] + 1, \"bar\": df[\"y\"] - df[\"x\"]})\n",
      "            >>> new_df.show()\n",
      "            ╭───────┬───────┬───────┬───────╮\n",
      "            │ x     ┆ y     ┆ foo   ┆ bar   │\n",
      "            │ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╪═══════╪═══════╡\n",
      "            │ 1     ┆ 4     ┆ 2     ┆ 3     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     ┆ 3     ┆ 3     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     ┆ 4     ┆ 3     │\n",
      "            ╰───────┴───────┴───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            columns (Dict[str, Expression]): Dictionary of new columns in the format { name: expression }\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with new columns.\n",
      "        \"\"\"\n",
      "        new_columns = [col.alias(name) for name, col in columns.items()]\n",
      "\n",
      "        builder = self._builder.with_columns(new_columns)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def with_column_renamed(self, existing: str, new: str) -> \"DataFrame\":\n",
      "        \"\"\"Renames a column in the current DataFrame.\n",
      "\n",
      "        If the column in the DataFrame schema does not exist, this will be a no-op.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n",
      "            >>> df.with_column_renamed(\"x\", \"foo\").show()\n",
      "            ╭───────┬───────╮\n",
      "            │ foo   ┆ y     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            existing (str): name of the existing column to rename\n",
      "            new (str): new name for the column\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the column renamed.\n",
      "        \"\"\"\n",
      "        builder = self._builder.with_column_renamed(existing, new)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def with_columns_renamed(self, cols_map: Dict[str, str]) -> \"DataFrame\":\n",
      "        \"\"\"Renames multiple columns in the current DataFrame.\n",
      "\n",
      "        If the columns in the DataFrame schema do not exist, this will be a no-op.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n",
      "            >>> df.with_columns_renamed({\"x\": \"foo\", \"y\": \"bar\"}).show()\n",
      "            ╭───────┬───────╮\n",
      "            │ foo   ┆ bar   │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            cols_map (Dict[str, str]): Dictionary of columns to rename in the format { existing: new }\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the columns renamed.\n",
      "        \"\"\"\n",
      "        builder = self._builder.with_columns_renamed(cols_map)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def sort(\n",
      "        self,\n",
      "        by: Union[ColumnInputType, List[ColumnInputType]],\n",
      "        desc: Union[bool, List[bool]] = False,\n",
      "        nulls_first: Optional[Union[bool, List[bool]]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Sorts DataFrame globally.\n",
      "\n",
      "        Note:\n",
      "            * Since this a global sort, this requires an expensive repartition which can be quite slow.\n",
      "            * Supports multicolumn sorts and can have unique `descending` flag per column.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [3, 2, 1], \"y\": [6, 4, 5]})\n",
      "            >>> sorted_df = df.sort(col(\"x\") + col(\"y\"))\n",
      "            >>> sorted_df.show()\n",
      "            ╭───────┬───────╮\n",
      "            │ x     ┆ y     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 2     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "            You can also sort by multiple columns, and specify the 'descending' flag for each column:\n",
      "\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 1, 2], \"y\": [9, 8, 7, 6]})\n",
      "            >>> sorted_df = df.sort([\"x\", \"y\"], [True, False])\n",
      "            >>> sorted_df.show()\n",
      "            ╭───────┬───────╮\n",
      "            │ x     ┆ y     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 2     ┆ 6     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 8     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1     ┆ 7     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1     ┆ 9     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 4 of 4 rows)\n",
      "\n",
      "        Args:\n",
      "            column (Union[ColumnInputType, List[ColumnInputType]]): column to sort by. Can be `str` or expression as well as a list of either.\n",
      "            desc (Union[bool, List[bool]), optional): Sort by descending order. Defaults to False.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Sorted DataFrame.\n",
      "        \"\"\"\n",
      "        if not isinstance(by, list):\n",
      "            by = [\n",
      "                by,\n",
      "            ]\n",
      "\n",
      "        if nulls_first is None:\n",
      "            nulls_first = desc\n",
      "\n",
      "        sort_by = self.__column_input_to_expression(by)\n",
      "\n",
      "        builder = self._builder.sort(sort_by=sort_by, descending=desc, nulls_first=nulls_first)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def limit(self, num: int) -> \"DataFrame\":\n",
      "        \"\"\"Limits the rows in the DataFrame to the first ``N`` rows, similar to a SQL ``LIMIT``.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = df = daft.from_pydict({\"x\": [1, 2, 3, 4, 5, 6, 7]})\n",
      "            >>> df_limited = df.limit(5)  # returns 5 rows\n",
      "            >>> df_limited.show()\n",
      "            ╭───────╮\n",
      "            │ x     │\n",
      "            │ ---   │\n",
      "            │ Int64 │\n",
      "            ╞═══════╡\n",
      "            │ 1     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 2     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 3     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 4     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 5     │\n",
      "            ╰───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 5 of 5 rows)\n",
      "\n",
      "        Args:\n",
      "            num (int): maximum rows to allow.\n",
      "            eager (bool): whether to maximize for latency (time to first result) by eagerly executing\n",
      "                only one partition at a time, or throughput by executing multiple limits at a time\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Limited DataFrame\n",
      "        \"\"\"\n",
      "        builder = self._builder.limit(num, eager=False)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def count_rows(self) -> int:\n",
      "        \"\"\"Executes the Dataframe to count the number of rows.\n",
      "\n",
      "        Returns:\n",
      "            int: count of the number of rows in this DataFrame.\n",
      "        \"\"\"\n",
      "        builder = self._builder.count()\n",
      "        count_df = DataFrame(builder)\n",
      "        # Expects builder to produce a single-partition, single-row DataFrame containing\n",
      "        # a \"count\" column, where the lone value represents the row count for the DataFrame.\n",
      "        return count_df.to_pydict()[\"count\"][0]\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def repartition(self, num: Optional[int], *partition_by: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Repartitions DataFrame to ``num`` partitions.\n",
      "\n",
      "        If columns are passed in, then DataFrame will be repartitioned by those, otherwise\n",
      "        random repartitioning will occur.\n",
      "\n",
      "        .. NOTE::\n",
      "\n",
      "            This function will globally shuffle your data, which is potentially a very expensive operation.\n",
      "\n",
      "            If instead you merely wish to \"split\" or \"coalesce\" partitions to obtain a target number of partitions,\n",
      "            you mean instead wish to consider using :meth:`DataFrame.into_partitions <daft.DataFrame.into_partitions>`\n",
      "            which avoids shuffling of data in favor of splitting/coalescing adjacent partitions where appropriate.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> repartitioned_df = df.repartition(3)\n",
      "            >>> repartitioned_df.num_partitions()\n",
      "            3\n",
      "\n",
      "        Args:\n",
      "            num (Optional[int]): Number of target partitions; if None, the number of partitions will not be changed.\n",
      "            *partition_by (Union[str, Expression]): Optional columns to partition by.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Repartitioned DataFrame.\n",
      "        \"\"\"\n",
      "        if len(partition_by) == 0:\n",
      "            warnings.warn(\n",
      "                \"No columns specified for repartition, so doing a random shuffle. If you do not require rebalancing of \"\n",
      "                \"partitions, you may instead prefer using `df.into_partitions(N)` which is a cheaper operation that \"\n",
      "                \"avoids shuffling data.\"\n",
      "            )\n",
      "            builder = self._builder.random_shuffle(num)\n",
      "        else:\n",
      "            builder = self._builder.hash_repartition(num, self.__column_input_to_expression(partition_by))\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def into_partitions(self, num: int) -> \"DataFrame\":\n",
      "        \"\"\"Splits or coalesces DataFrame to ``num`` partitions. Order is preserved.\n",
      "\n",
      "        This will naively greedily split partitions in a round-robin fashion to hit the targeted number of partitions.\n",
      "        The number of rows/size in a given partition is not taken into account during the splitting.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> df_with_5_partitions = df.into_partitions(5)\n",
      "            >>> df_with_5_partitions.num_partitions()\n",
      "            5\n",
      "\n",
      "        Args:\n",
      "            num (int): number of target partitions.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Dataframe with ``num`` partitions.\n",
      "        \"\"\"\n",
      "        builder = self._builder.into_partitions(num)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def join(\n",
      "        self,\n",
      "        other: \"DataFrame\",\n",
      "        on: Optional[Union[List[ColumnInputType], ColumnInputType]] = None,\n",
      "        left_on: Optional[Union[List[ColumnInputType], ColumnInputType]] = None,\n",
      "        right_on: Optional[Union[List[ColumnInputType], ColumnInputType]] = None,\n",
      "        how: Literal[\"inner\", \"inner\", \"left\", \"right\", \"outer\", \"anti\", \"semi\", \"cross\"] = \"inner\",\n",
      "        strategy: Optional[Literal[\"hash\", \"sort_merge\", \"broadcast\"]] = None,\n",
      "        prefix: Optional[str] = None,\n",
      "        suffix: Optional[str] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Column-wise join of the current DataFrame with an ``other`` DataFrame, similar to a SQL ``JOIN``.\n",
      "\n",
      "        If the two DataFrames have duplicate non-join key column names, \"right.\" will be prepended to the conflicting right columns. You can change the behavior by passing either (or both) `prefix` or `suffix` to the function.\n",
      "        If `prefix` is passed, it will be prepended to the conflicting right columns. If `suffix` is passed, it will be appended to the conflicting right columns.\n",
      "\n",
      "        .. NOTE::\n",
      "            Although self joins are supported, we currently duplicate the logical plan for the right side\n",
      "            and recompute the entire tree. Caching for this is on the roadmap.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df1 = daft.from_pydict({\"a\": [\"w\", \"x\", \"y\"], \"b\": [1, 2, 3]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [\"x\", \"y\", \"z\"], \"b\": [20, 30, 40]})\n",
      "            >>> joined_df = df1.join(df2, left_on=[col(\"a\"), col(\"b\")], right_on=[col(\"a\"), col(\"b\") / 10])\n",
      "            >>> joined_df.show()\n",
      "            ╭──────┬───────┬─────────╮\n",
      "            │ a    ┆ b     ┆ right.b │\n",
      "            │ ---  ┆ ---   ┆ ---     │\n",
      "            │ Utf8 ┆ Int64 ┆ Int64   │\n",
      "            ╞══════╪═══════╪═════════╡\n",
      "            │ x    ┆ 2     ┆ 20      │\n",
      "            ├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "            │ y    ┆ 3     ┆ 30      │\n",
      "            ╰──────┴───────┴─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df1 = daft.from_pydict({\"a\": [\"w\", \"x\", \"y\"], \"b\": [1, 2, 3]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [\"x\", \"y\", \"z\"], \"b\": [20, 30, 40]})\n",
      "            >>> joined_df = df1.join(df2, left_on=[col(\"a\"), col(\"b\")], right_on=[col(\"a\"), col(\"b\") / 10], prefix=\"right_\")\n",
      "            >>> joined_df.show()\n",
      "            ╭──────┬───────┬─────────╮\n",
      "            │ a    ┆ b     ┆ right_b │\n",
      "            │ ---  ┆ ---   ┆ ---     │\n",
      "            │ Utf8 ┆ Int64 ┆ Int64   │\n",
      "            ╞══════╪═══════╪═════════╡\n",
      "            │ x    ┆ 2     ┆ 20      │\n",
      "            ├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "            │ y    ┆ 3     ┆ 30      │\n",
      "            ╰──────┴───────┴─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df1 = daft.from_pydict({\"a\": [\"w\", \"x\", \"y\"], \"b\": [1, 2, 3]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [\"x\", \"y\", \"z\"], \"b\": [20, 30, 40]})\n",
      "            >>> joined_df = df1.join(df2, left_on=[col(\"a\"), col(\"b\")], right_on=[col(\"a\"), col(\"b\") / 10], suffix=\"_right\")\n",
      "            >>> joined_df.show()\n",
      "            ╭──────┬───────┬─────────╮\n",
      "            │ a    ┆ b     ┆ b_right │\n",
      "            │ ---  ┆ ---   ┆ ---     │\n",
      "            │ Utf8 ┆ Int64 ┆ Int64   │\n",
      "            ╞══════╪═══════╪═════════╡\n",
      "            │ x    ┆ 2     ┆ 20      │\n",
      "            ├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "            │ y    ┆ 3     ┆ 30      │\n",
      "            ╰──────┴───────┴─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): the right DataFrame to join on.\n",
      "            on (Optional[Union[List[ColumnInputType], ColumnInputType]], optional): key or keys to join on [use if the keys on the left and right side match.]. Defaults to None.\n",
      "            left_on (Optional[Union[List[ColumnInputType], ColumnInputType]], optional): key or keys to join on left DataFrame. Defaults to None.\n",
      "            right_on (Optional[Union[List[ColumnInputType], ColumnInputType]], optional): key or keys to join on right DataFrame. Defaults to None.\n",
      "            how (str, optional): what type of join to perform; currently \"inner\", \"left\", \"right\", \"outer\", \"anti\", \"semi\", and \"cross\" are supported. Defaults to \"inner\".\n",
      "            strategy (Optional[str]): The join strategy (algorithm) to use; currently \"hash\", \"sort_merge\", \"broadcast\", and None are supported, where None\n",
      "                chooses the join strategy automatically during query optimization. The default is None.\n",
      "            suffix (Optional[str], optional): Suffix to add to the column names in case of a name collision. Defaults to \"\".\n",
      "            prefix (Optional[str], optional): Prefix to add to the column names in case of a name collision. Defaults to \"right.\".\n",
      "\n",
      "        Raises:\n",
      "            ValueError: if `on` is passed in and `left_on` or `right_on` is not None.\n",
      "            ValueError: if `on` is None but both `left_on` and `right_on` are not defined.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Joined DataFrame.\n",
      "        \"\"\"\n",
      "        if how == \"cross\":\n",
      "            if any(side_on is not None for side_on in [on, left_on, right_on]):\n",
      "                raise ValueError(\"In a cross join, `on`, `left_on`, and `right_on` cannot be set\")\n",
      "\n",
      "            left_on = []\n",
      "            right_on = []\n",
      "        elif on is None:\n",
      "            if left_on is None or right_on is None:\n",
      "                raise ValueError(\"If `on` is None then both `left_on` and `right_on` must not be None\")\n",
      "        else:\n",
      "            if left_on is not None or right_on is not None:\n",
      "                raise ValueError(\"If `on` is not None then both `left_on` and `right_on` must be None\")\n",
      "            left_on = on\n",
      "            right_on = on\n",
      "\n",
      "        join_type = JoinType.from_join_type_str(how)\n",
      "        join_strategy = JoinStrategy.from_join_strategy_str(strategy) if strategy is not None else None\n",
      "\n",
      "        if join_strategy == JoinStrategy.SortMerge and join_type != JoinType.Inner:\n",
      "            raise ValueError(\"Sort merge join only supports inner joins\")\n",
      "        elif join_strategy == JoinStrategy.Broadcast and join_type == JoinType.Outer:\n",
      "            raise ValueError(\"Broadcast join does not support outer joins\")\n",
      "\n",
      "        left_exprs = self.__column_input_to_expression(tuple(left_on) if isinstance(left_on, list) else (left_on,))\n",
      "        right_exprs = self.__column_input_to_expression(tuple(right_on) if isinstance(right_on, list) else (right_on,))\n",
      "        builder = self._builder.join(\n",
      "            other._builder,\n",
      "            left_on=left_exprs,\n",
      "            right_on=right_exprs,\n",
      "            how=join_type,\n",
      "            strategy=join_strategy,\n",
      "            prefix=prefix,\n",
      "            suffix=suffix,\n",
      "        )\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def concat(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Concatenates two DataFrames together in a \"vertical\" concatenation.\n",
      "\n",
      "        The resulting DataFrame\n",
      "        has number of rows equal to the sum of the number of rows of the input DataFrames.\n",
      "\n",
      "        .. NOTE::\n",
      "            DataFrames being concatenated **must have exactly the same schema**. You may wish to use the\n",
      "            :meth:`df.select() <daft.DataFrame.select>` and :meth:`expr.cast() <daft.Expression.cast>` methods\n",
      "            to ensure schema compatibility before concatenation.\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): other DataFrame to concatenate\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with rows from `self` on top and rows from `other` at the bottom.\n",
      "        \"\"\"\n",
      "        if self.schema() != other.schema():\n",
      "            raise ValueError(\n",
      "                f\"DataFrames must have exactly the same schema for concatenation!\\nExpected:\\n{self.schema()}\\n\\nReceived:\\n{other.schema()}\"\n",
      "            )\n",
      "        builder = self._builder.concat(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def drop_nan(self, *cols: ColumnInputType):\n",
      "        \"\"\"Drops rows that contains NaNs. If cols is None it will drop rows with any NaN value.\n",
      "\n",
      "        If column names are supplied, it will drop only those rows that contains NaNs in one of these columns.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1.0, 2.2, 3.5, float(\"nan\")]})\n",
      "            >>> df.drop_nan().collect()  # drops rows where any column contains NaN values\n",
      "            ╭─────────╮\n",
      "            │ a       │\n",
      "            │ ---     │\n",
      "            │ Float64 │\n",
      "            ╞═════════╡\n",
      "            │ 1       │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2.2     │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 3.5     │\n",
      "            ╰─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1.6, 2.5, 3.3, float(\"nan\")]})\n",
      "            >>> df.drop_nan(\"a\").collect()  # drops rows where column a contains NaN values\n",
      "            ╭─────────╮\n",
      "            │ a       │\n",
      "            │ ---     │\n",
      "            │ Float64 │\n",
      "            ╞═════════╡\n",
      "            │ 1.6     │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2.5     │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 3.3     │\n",
      "            ╰─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            *cols (str): column names by which rows containing nans/NULLs should be filtered\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame without NaNs in specified/all columns\n",
      "\n",
      "        \"\"\"\n",
      "        if len(cols) == 0:\n",
      "            columns = self.__column_input_to_expression(self.column_names)\n",
      "        else:\n",
      "            columns = self.__column_input_to_expression(cols)\n",
      "        float_columns = [\n",
      "            column\n",
      "            for column in columns\n",
      "            if (\n",
      "                column._to_field(self.schema()).dtype == DataType.float32()\n",
      "                or column._to_field(self.schema()).dtype == DataType.float64()\n",
      "            )\n",
      "        ]\n",
      "\n",
      "        return self.where(\n",
      "            ~reduce(\n",
      "                lambda x, y: x.is_null().if_else(lit(False), x) | y.is_null().if_else(lit(False), y),\n",
      "                (x.float.is_nan() for x in float_columns),\n",
      "            )\n",
      "        )\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def drop_null(self, *cols: ColumnInputType):\n",
      "        \"\"\"Drops rows that contains NaNs or NULLs. If cols is None it will drop rows with any NULL value.\n",
      "\n",
      "        If column names are supplied, it will drop only those rows that contains NULLs in one of these columns.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"a\": [1.6, 2.5, None, float(\"NaN\")]})\n",
      "            >>> df.drop_null(\"a\").collect()\n",
      "            ╭─────────╮\n",
      "            │ a       │\n",
      "            │ ---     │\n",
      "            │ Float64 │\n",
      "            ╞═════════╡\n",
      "            │ 1.6     │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2.5     │\n",
      "            ├╌╌╌╌╌╌╌╌╌┤\n",
      "            │ NaN     │\n",
      "            ╰─────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            *cols (str): column names by which rows containing nans should be filtered\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame without missing values in specified/all columns\n",
      "        \"\"\"\n",
      "        if len(cols) == 0:\n",
      "            columns = self.__column_input_to_expression(self.column_names)\n",
      "        else:\n",
      "            columns = self.__column_input_to_expression(cols)\n",
      "        return self.where(~reduce(lambda x, y: x | y, (x.is_null() for x in columns)))\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def explode(self, *columns: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Explodes a List column, where every element in each row's List becomes its own row, and all other columns in the DataFrame are duplicated across rows.\n",
      "\n",
      "        If multiple columns are specified, each row must contain the same number of\n",
      "        items in each specified column.\n",
      "\n",
      "        Exploding Null values or empty lists will create a single Null entry (see example below).\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict(\n",
      "            ...     {\n",
      "            ...         \"x\": [[1], [2, 3]],\n",
      "            ...         \"y\": [[\"a\"], [\"b\", \"c\"]],\n",
      "            ...         \"z\": [\n",
      "            ...             [1.0],\n",
      "            ...             [2.0, 2.0],\n",
      "            ...         ],\n",
      "            ...     }\n",
      "            ... )\n",
      "            >>> df.explode(col(\"x\"), col(\"y\")).collect()\n",
      "            ╭───────┬──────┬───────────────╮\n",
      "            │ x     ┆ y    ┆ z             │\n",
      "            │ ---   ┆ ---  ┆ ---           │\n",
      "            │ Int64 ┆ Utf8 ┆ List[Float64] │\n",
      "            ╞═══════╪══════╪═══════════════╡\n",
      "            │ 1     ┆ a    ┆ [1]           │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ b    ┆ [2, 2]        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ c    ┆ [2, 2]        │\n",
      "            ╰───────┴──────┴───────────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            *columns (ColumnInputType): columns to explode\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with exploded column\n",
      "        \"\"\"\n",
      "        parsed_exprs = self.__column_input_to_expression(columns)\n",
      "        builder = self._builder.explode(parsed_exprs)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def unpivot(\n",
      "        self,\n",
      "        ids: ManyColumnsInputType,\n",
      "        values: ManyColumnsInputType = [],\n",
      "        variable_name: str = \"variable\",\n",
      "        value_name: str = \"value\",\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Unpivots a DataFrame from wide to long format.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict(\n",
      "            ...     {\n",
      "            ...         \"year\": [2020, 2021, 2022],\n",
      "            ...         \"Jan\": [10, 30, 50],\n",
      "            ...         \"Feb\": [20, 40, 60],\n",
      "            ...     }\n",
      "            ... )\n",
      "            >>> df = df.unpivot(\"year\", [\"Jan\", \"Feb\"], variable_name=\"month\", value_name=\"inventory\")\n",
      "            >>> df = df.sort(\"year\")\n",
      "            >>> df.show()\n",
      "            ╭───────┬───────┬───────────╮\n",
      "            │ year  ┆ month ┆ inventory │\n",
      "            │ ---   ┆ ---   ┆ ---       │\n",
      "            │ Int64 ┆ Utf8  ┆ Int64     │\n",
      "            ╞═══════╪═══════╪═══════════╡\n",
      "            │ 2020  ┆ Jan   ┆ 10        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2020  ┆ Feb   ┆ 20        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2021  ┆ Jan   ┆ 30        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2021  ┆ Feb   ┆ 40        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2022  ┆ Jan   ┆ 50        │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "            │ 2022  ┆ Feb   ┆ 60        │\n",
      "            ╰───────┴───────┴───────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 6 of 6 rows)\n",
      "\n",
      "        Args:\n",
      "            ids (ManyColumnsInputType): Columns to keep as identifiers\n",
      "            values (Optional[ManyColumnsInputType]): Columns to unpivot. If not specified, all columns except ids will be unpivoted.\n",
      "            variable_name (Optional[str]): Name of the variable column. Defaults to \"variable\".\n",
      "            value_name (Optional[str]): Name of the value column. Defaults to \"value\".\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Unpivoted DataFrame\n",
      "\n",
      "        See Also:\n",
      "            `melt`\n",
      "        \"\"\"\n",
      "        ids_exprs = self._column_inputs_to_expressions(ids)\n",
      "        values_exprs = self._column_inputs_to_expressions(values)\n",
      "\n",
      "        builder = self._builder.unpivot(ids_exprs, values_exprs, variable_name, value_name)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def melt(\n",
      "        self,\n",
      "        ids: ManyColumnsInputType,\n",
      "        values: ManyColumnsInputType = [],\n",
      "        variable_name: str = \"variable\",\n",
      "        value_name: str = \"value\",\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Alias for unpivot.\n",
      "\n",
      "        See Also:\n",
      "            `unpivot`\n",
      "        \"\"\"\n",
      "        return self.unpivot(ids, values, variable_name, value_name)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def transform(self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any) -> \"DataFrame\":\n",
      "        \"\"\"Apply a function that takes and returns a DataFrame.\n",
      "\n",
      "        Allow splitting your transformation into different units of work (functions) while preserving the syntax for chaining transformations.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"col_a\": [1, 2, 3, 4]})\n",
      "            >>> def add_1(df):\n",
      "            ...     df = df.select(daft.col(\"col_a\") + 1)\n",
      "            ...     return df\n",
      "            >>> def multiply_x(df, x):\n",
      "            ...     df = df.select(daft.col(\"col_a\") * x)\n",
      "            ...     return df\n",
      "            >>> df = df.transform(add_1).transform(multiply_x, 4)\n",
      "            >>> df.show()\n",
      "            ╭───────╮\n",
      "            │ col_a │\n",
      "            │ ---   │\n",
      "            │ Int64 │\n",
      "            ╞═══════╡\n",
      "            │ 8     │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 12    │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 16    │\n",
      "            ├╌╌╌╌╌╌╌┤\n",
      "            │ 20    │\n",
      "            ╰───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 4 of 4 rows)\n",
      "\n",
      "        Args:\n",
      "            func: A function that takes and returns a DataFrame.\n",
      "            *args: Positional arguments to pass to func.\n",
      "            **kwargs: Keyword arguments to pass to func.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Transformed DataFrame.\n",
      "        \"\"\"\n",
      "        result = func(self, *args, **kwargs)\n",
      "        assert isinstance(\n",
      "            result, DataFrame\n",
      "        ), f\"Func returned an instance of type [{type(result)}], should have been DataFrame.\"\n",
      "        return result\n",
      "\n",
      "    def _agg(\n",
      "        self,\n",
      "        to_agg: Iterable[Expression],\n",
      "        group_by: Optional[ExpressionsProjection] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        builder = self._builder.agg(list(to_agg), list(group_by) if group_by is not None else None)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    def _map_agg_string_to_expr(self, expr: Expression, op: str) -> Expression:\n",
      "        if op == \"sum\":\n",
      "            return expr.sum()\n",
      "        elif op == \"count\":\n",
      "            return expr.count()\n",
      "        elif op == \"min\":\n",
      "            return expr.min()\n",
      "        elif op == \"max\":\n",
      "            return expr.max()\n",
      "        elif op == \"mean\":\n",
      "            return expr.mean()\n",
      "        elif op == \"any_value\":\n",
      "            return expr.any_value()\n",
      "        elif op == \"list\":\n",
      "            return expr.agg_list()\n",
      "        elif op == \"set\":\n",
      "            return expr.agg_set()\n",
      "        elif op == \"concat\":\n",
      "            return expr.agg_concat()\n",
      "\n",
      "        raise NotImplementedError(f\"Aggregation {op} is not implemented.\")\n",
      "\n",
      "    def _apply_agg_fn(\n",
      "        self,\n",
      "        fn: Callable[[Expression], Expression],\n",
      "        cols: Tuple[ManyColumnsInputType, ...],\n",
      "        group_by: Optional[ExpressionsProjection] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        if len(cols) == 0:\n",
      "            warnings.warn(\"No columns specified; performing aggregation on all columns.\")\n",
      "\n",
      "            groupby_name_set = set() if group_by is None else group_by.to_name_set()\n",
      "            cols = tuple(c for c in self.column_names if c not in groupby_name_set)\n",
      "        exprs = self._wildcard_inputs_to_expressions(cols)\n",
      "        return self._agg([fn(c) for c in exprs], group_by)\n",
      "\n",
      "    def _map_groups(self, udf: Expression, group_by: Optional[ExpressionsProjection] = None) -> \"DataFrame\":\n",
      "        builder = self._builder.map_groups(udf, list(group_by) if group_by is not None else None)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def sum(self, *cols: ManyColumnsInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global sum on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to sum\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated sums. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.sum, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def mean(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global mean on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to mean\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated mean. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.mean, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def stddev(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global standard deviation on the DataFrame.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"col_a\": [0, 1, 2]})\n",
      "            >>> df = df.stddev(\"col_a\")\n",
      "            >>> df.show()\n",
      "            ╭───────────────────╮\n",
      "            │ col_a             │\n",
      "            │ ---               │\n",
      "            │ Float64           │\n",
      "            ╞═══════════════════╡\n",
      "            │ 0.816496580927726 │\n",
      "            ╰───────────────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to stddev\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated standard deviation. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.stddev, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def min(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global min on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to min\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated min. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.min, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def max(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global max on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to max\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated max. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.max, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def any_value(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Returns an arbitrary value on this DataFrame.\n",
      "\n",
      "        Values for each column are not guaranteed to be from the same row.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to get an arbitrary value from\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with any values.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.any_value, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def count(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global count on the DataFrame.\n",
      "\n",
      "        If no columns are specified (i.e. in the case you call `df.count()`), or only the literal string \"*\",\n",
      "        this functions very similarly to a COUNT(*) operation in SQL and will return a new dataframe with a\n",
      "        single column with the name \"count\".\n",
      "\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df = daft.from_pydict({\"foo\": [1, None, None], \"bar\": [None, 2, 2], \"baz\": [3, 4, 5]})\n",
      "            >>> df.count().show()  # equivalent to df.count(\"*\").show()\n",
      "            ╭────────╮\n",
      "            │ count  │\n",
      "            │ ---    │\n",
      "            │ UInt64 │\n",
      "            ╞════════╡\n",
      "            │ 3      │\n",
      "            ╰────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        However, specifying some column names would instead change the behavior to count all non-null values,\n",
      "        similar to a SQL command for `SELECT COUNT(foo), COUNT(bar) FROM df`. Also, using `df.count(col(\"*\"))`\n",
      "        will expand out into count() for each column.\n",
      "\n",
      "            >>> df.count(\"foo\", \"bar\").show()\n",
      "            ╭────────┬────────╮\n",
      "            │ foo    ┆ bar    │\n",
      "            │ ---    ┆ ---    │\n",
      "            │ UInt64 ┆ UInt64 │\n",
      "            ╞════════╪════════╡\n",
      "            │ 1      ┆ 2      │\n",
      "            ╰────────┴────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "            >>> df.count(col(\"*\")).show()\n",
      "            ╭────────┬────────┬────────╮\n",
      "            │ foo    ┆ bar    ┆ baz    │\n",
      "            │ ---    ┆ ---    ┆ ---    │\n",
      "            │ UInt64 ┆ UInt64 ┆ UInt64 │\n",
      "            ╞════════╪════════╪════════╡\n",
      "            │ 1      ┆ 2      ┆ 3      │\n",
      "            ╰────────┴────────┴────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to count\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated count. Should be a single row.\n",
      "        \"\"\"\n",
      "        # Special case: treat this as a COUNT(*) operation which is likely what most people would expect\n",
      "        # If user passes in \"*\", also do this behavior (by default it would count each column individually)\n",
      "        if len(cols) == 0 or (len(cols) == 1 and isinstance(cols[0], str) and cols[0] == \"*\"):\n",
      "            builder = self._builder.count()\n",
      "            return DataFrame(builder)\n",
      "\n",
      "        if any(isinstance(c, str) and c == \"*\" for c in cols):\n",
      "            # we do not support hybrid count-all and count-nonnull\n",
      "            raise ValueError(\"Cannot call count() with both * and column names\")\n",
      "\n",
      "        # Otherwise, perform a column-wise count on the specified columns\n",
      "        return self._apply_agg_fn(Expression.count, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def agg_list(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global list agg on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to form into a list\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated list. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.agg_list, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def agg_set(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global set agg on the DataFrame (ignoring nulls).\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns to form into a set\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated set. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.agg_set, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def agg_concat(self, *cols: ColumnInputType) -> \"DataFrame\":\n",
      "        \"\"\"Performs a global list concatenation agg on the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            *cols (Union[str, Expression]): columns that are lists to concatenate\n",
      "        Returns:\n",
      "            DataFrame: Globally aggregated list. Should be a single row.\n",
      "        \"\"\"\n",
      "        return self._apply_agg_fn(Expression.agg_concat, cols)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def agg(self, *to_agg: Union[Expression, Iterable[Expression]]) -> \"DataFrame\":\n",
      "        \"\"\"Perform aggregations on this DataFrame.\n",
      "\n",
      "        Allows for mixed aggregations for multiple columns.\n",
      "        Will return a single row that aggregated the entire DataFrame.\n",
      "\n",
      "        For a full list of aggregation expressions, see :ref:`Aggregation Expressions <api=aggregation-expression>`\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df = daft.from_pydict(\n",
      "            ...     {\"student_id\": [1, 2, 3, 4], \"test1\": [0.5, 0.4, 0.6, 0.7], \"test2\": [0.9, 0.8, 0.7, 1.0]}\n",
      "            ... )\n",
      "            >>> agg_df = df.agg(\n",
      "            ...     col(\"test1\").mean(),\n",
      "            ...     col(\"test2\").mean(),\n",
      "            ...     ((col(\"test1\") + col(\"test2\")) / 2).min().alias(\"total_min\"),\n",
      "            ...     ((col(\"test1\") + col(\"test2\")) / 2).max().alias(\"total_max\"),\n",
      "            ... )\n",
      "            >>> agg_df.show()\n",
      "            ╭─────────┬────────────────────┬────────────────────┬───────────╮\n",
      "            │ test1   ┆ test2              ┆ total_min          ┆ total_max │\n",
      "            │ ---     ┆ ---                ┆ ---                ┆ ---       │\n",
      "            │ Float64 ┆ Float64            ┆ Float64            ┆ Float64   │\n",
      "            ╞═════════╪════════════════════╪════════════════════╪═══════════╡\n",
      "            │ 0.55    ┆ 0.8500000000000001 ┆ 0.6000000000000001 ┆ 0.85      │\n",
      "            ╰─────────┴────────────────────┴────────────────────┴───────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        Args:\n",
      "            *to_agg (Expression): aggregation expressions\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with aggregated results\n",
      "        \"\"\"\n",
      "        to_agg_list = (\n",
      "            list(to_agg[0])\n",
      "            if (len(to_agg) == 1 and not isinstance(to_agg[0], Expression))\n",
      "            else list(typing.cast(\"Tuple[Expression]\", to_agg))\n",
      "        )\n",
      "\n",
      "        for expr in to_agg_list:\n",
      "            if not isinstance(expr, Expression):\n",
      "                raise ValueError(f\"DataFrame.agg() only accepts expression type, received: {type(expr)}\")\n",
      "\n",
      "        return self._agg(to_agg_list, group_by=None)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def groupby(self, *group_by: ManyColumnsInputType) -> \"GroupedDataFrame\":\n",
      "        \"\"\"Performs a GroupBy on the DataFrame for aggregation.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df = daft.from_pydict(\n",
      "            ...     {\n",
      "            ...         \"pet\": [\"cat\", \"dog\", \"dog\", \"cat\"],\n",
      "            ...         \"age\": [1, 2, 3, 4],\n",
      "            ...         \"name\": [\"Alex\", \"Jordan\", \"Sam\", \"Riley\"],\n",
      "            ...     }\n",
      "            ... )\n",
      "            >>> grouped_df = df.groupby(\"pet\").agg(\n",
      "            ...     col(\"age\").min().alias(\"min_age\"),\n",
      "            ...     col(\"age\").max().alias(\"max_age\"),\n",
      "            ...     col(\"pet\").count().alias(\"count\"),\n",
      "            ...     col(\"name\").any_value(),\n",
      "            ... )\n",
      "            >>> grouped_df.show()\n",
      "            ╭──────┬─────────┬─────────┬────────┬────────╮\n",
      "            │ pet  ┆ min_age ┆ max_age ┆ count  ┆ name   │\n",
      "            │ ---  ┆ ---     ┆ ---     ┆ ---    ┆ ---    │\n",
      "            │ Utf8 ┆ Int64   ┆ Int64   ┆ UInt64 ┆ Utf8   │\n",
      "            ╞══════╪═════════╪═════════╪════════╪════════╡\n",
      "            │ cat  ┆ 1       ┆ 4       ┆ 2      ┆ Alex   │\n",
      "            ├╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤\n",
      "            │ dog  ┆ 2       ┆ 3       ┆ 2      ┆ Jordan │\n",
      "            ╰──────┴─────────┴─────────┴────────┴────────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Args:\n",
      "            *group_by (Union[str, Expression]): columns to group by\n",
      "\n",
      "        Returns:\n",
      "            GroupedDataFrame: DataFrame to Aggregate\n",
      "        \"\"\"\n",
      "        return GroupedDataFrame(self, ExpressionsProjection(self._wildcard_inputs_to_expressions(group_by)))\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def pivot(\n",
      "        self,\n",
      "        group_by: ManyColumnsInputType,\n",
      "        pivot_col: ColumnInputType,\n",
      "        value_col: ColumnInputType,\n",
      "        agg_fn: str,\n",
      "        names: Optional[List[str]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Pivots a column of the DataFrame and performs an aggregation on the values.\n",
      "\n",
      "        .. NOTE::\n",
      "            You may wish to provide a list of distinct values to pivot on, which is more efficient as it avoids\n",
      "            a distinct operation. Without this list, Daft will perform a distinct operation on the pivot column to\n",
      "            determine the unique values to pivot on.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> data = {\n",
      "            ...     \"id\": [1, 2, 3, 4],\n",
      "            ...     \"version\": [\"3.8\", \"3.8\", \"3.9\", \"3.9\"],\n",
      "            ...     \"platform\": [\"macos\", \"macos\", \"macos\", \"windows\"],\n",
      "            ...     \"downloads\": [100, 200, 150, 250],\n",
      "            ... }\n",
      "            >>> df = daft.from_pydict(data)\n",
      "            >>> df = df.pivot(\"version\", \"platform\", \"downloads\", \"sum\")\n",
      "            >>> df.show()\n",
      "            ╭─────────┬─────────┬───────╮\n",
      "            │ version ┆ windows ┆ macos │\n",
      "            │ ---     ┆ ---     ┆ ---   │\n",
      "            │ Utf8    ┆ Int64   ┆ Int64 │\n",
      "            ╞═════════╪═════════╪═══════╡\n",
      "            │ 3.9     ┆ 250     ┆ 150   │\n",
      "            ├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3.8     ┆ None    ┆ 300   │\n",
      "            ╰─────────┴─────────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Args:\n",
      "            group_by (ManyColumnsInputType): columns to group by\n",
      "            pivot_col (Union[str, Expression]): column to pivot\n",
      "            value_col (Union[str, Expression]): column to aggregate\n",
      "            agg_fn (str): aggregation function to apply\n",
      "            names (Optional[List[str]]): names of the pivoted columns\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with pivoted columns\n",
      "\n",
      "        \"\"\"\n",
      "        group_by_expr = self._column_inputs_to_expressions(group_by)\n",
      "        [pivot_col_expr, value_col_expr] = self._column_inputs_to_expressions([pivot_col, value_col])\n",
      "        agg_expr = self._map_agg_string_to_expr(value_col_expr, agg_fn)\n",
      "\n",
      "        if names is None:\n",
      "            names = self.select(pivot_col_expr).distinct().to_pydict()[pivot_col_expr.name()]\n",
      "            names = [str(x) for x in names]\n",
      "        builder = self._builder.pivot(group_by_expr, pivot_col_expr, value_col_expr, agg_expr, names)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def union(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the distinct union of two DataFrames.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"x\": [3, 4, 5], \"y\": [6, 7, 8]})\n",
      "            >>> df1.union(df2).sort(\"x\").show()\n",
      "            ╭───────┬───────╮\n",
      "            │ x     ┆ y     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 4     ┆ 7     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 5     ┆ 8     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 5 of 5 rows)\n",
      "        \"\"\"\n",
      "        builder = self._builder.union(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def union_all(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the union of two DataFrames, including duplicates.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"x\": [3, 2, 1], \"y\": [6, 5, 4]})\n",
      "            >>> df1.union_all(df2).sort(\"x\").show()\n",
      "            ╭───────┬───────╮\n",
      "            │ x     ┆ y     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 6 of 6 rows)\n",
      "        \"\"\"\n",
      "        builder = self._builder.union(other._builder, is_all=True)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def union_by_name(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the distinct union by name.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"x\": [1, 2], \"y\": [4, 5], \"w\": [9, 10]})\n",
      "            >>> df2 = daft.from_pydict({\"y\": [6, 7], \"z\": [\"a\", \"b\"]})\n",
      "            >>> df1.union_by_name(df2).sort(\"y\").show()\n",
      "            ╭───────┬───────┬───────┬──────╮\n",
      "            │ x     ┆ y     ┆ w     ┆ z    │\n",
      "            │ ---   ┆ ---   ┆ ---   ┆ ---  │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 ┆ Utf8 │\n",
      "            ╞═══════╪═══════╪═══════╪══════╡\n",
      "            │ 1     ┆ 4     ┆ 9     ┆ None │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     ┆ 10    ┆ None │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 6     ┆ None  ┆ a    │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 7     ┆ None  ┆ b    │\n",
      "            ╰───────┴───────┴───────┴──────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 4 of 4 rows)\n",
      "        \"\"\"\n",
      "        builder = self._builder.union(other._builder, is_all=False, is_by_name=True)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def union_all_by_name(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the union of two DataFrames, including duplicates, with columns matched by name.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"x\": [1, 2], \"y\": [4, 5], \"w\": [9, 10]})\n",
      "            >>> df2 = daft.from_pydict({\"y\": [6, 6, 7, 7], \"z\": [\"a\", \"a\", \"b\", \"b\"]})\n",
      "            >>> df1.union_all_by_name(df2).sort(\"y\").show()\n",
      "            ╭───────┬───────┬───────┬──────╮\n",
      "            │ x     ┆ y     ┆ w     ┆ z    │\n",
      "            │ ---   ┆ ---   ┆ ---   ┆ ---  │\n",
      "            │ Int64 ┆ Int64 ┆ Int64 ┆ Utf8 │\n",
      "            ╞═══════╪═══════╪═══════╪══════╡\n",
      "            │ 1     ┆ 4     ┆ 9     ┆ None │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 5     ┆ 10    ┆ None │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 6     ┆ None  ┆ a    │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 6     ┆ None  ┆ a    │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 7     ┆ None  ┆ b    │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n",
      "            │ None  ┆ 7     ┆ None  ┆ b    │\n",
      "            ╰───────┴───────┴───────┴──────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 6 of 6 rows)\n",
      "        \"\"\"\n",
      "        builder = self._builder.union(other._builder, is_all=True, is_by_name=True)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def intersect(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the intersection of two DataFrames.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [1, 2, 3], \"b\": [4, 8, 6]})\n",
      "            >>> df1.intersect(df2).collect()\n",
      "            ╭───────┬───────╮\n",
      "            │ a     ┆ b     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 3     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 2 of 2 rows)\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): DataFrame to intersect with\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the intersection of the two DataFrames\n",
      "        \"\"\"\n",
      "        builder = self._builder.intersect(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def intersect_all(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the intersection of two DataFrames, including duplicates.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"a\": [1, 2, 2], \"b\": [4, 6, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [1, 1, 2, 2], \"b\": [4, 4, 6, 6]})\n",
      "            >>> df1.intersect_all(df2).sort(\"a\").collect()\n",
      "            ╭───────┬───────╮\n",
      "            │ a     ┆ b     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 6     │\n",
      "            ├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "            │ 2     ┆ 6     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 3 of 3 rows)\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): DataFrame to intersect with\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the intersection of the two DataFrames, including duplicates\n",
      "        \"\"\"\n",
      "        builder = self._builder.intersect_all(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def except_distinct(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the set difference of two DataFrames.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [1, 2, 3], \"b\": [4, 8, 6]})\n",
      "            >>> df1.except_distinct(df2).collect()\n",
      "            ╭───────┬───────╮\n",
      "            │ a     ┆ b     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 2     ┆ 5     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): DataFrame to except with\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the set difference of the two DataFrames\n",
      "        \"\"\"\n",
      "        builder = self._builder.except_distinct(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def except_all(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the set difference of two DataFrames, considering duplicates.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df1 = daft.from_pydict({\"a\": [1, 1, 2, 2], \"b\": [4, 4, 6, 6]})\n",
      "            >>> df2 = daft.from_pydict({\"a\": [1, 2, 2], \"b\": [4, 6, 6]})\n",
      "            >>> df1.except_all(df2).collect()\n",
      "            ╭───────┬───────╮\n",
      "            │ a     ┆ b     │\n",
      "            │ ---   ┆ ---   │\n",
      "            │ Int64 ┆ Int64 │\n",
      "            ╞═══════╪═══════╡\n",
      "            │ 1     ┆ 4     │\n",
      "            ╰───────┴───────╯\n",
      "            <BLANKLINE>\n",
      "            (Showing first 1 of 1 rows)\n",
      "\n",
      "        Args:\n",
      "            other (DataFrame): DataFrame to except with\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with the set difference of the two DataFrames, considering duplicates\n",
      "        \"\"\"\n",
      "        builder = self._builder.except_all(other._builder)\n",
      "        return DataFrame(builder)\n",
      "\n",
      "    def _materialize_results(self) -> None:\n",
      "        \"\"\"Materializes the results of for this DataFrame and hold a pointer to the results.\"\"\"\n",
      "        context = get_context()\n",
      "        if self._result is None:\n",
      "            self._result_cache = context.get_or_create_runner().run(self._builder)\n",
      "            result = self._result\n",
      "            assert result is not None\n",
      "            result.wait()\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def collect(self, num_preview_rows: Optional[int] = 8) -> \"DataFrame\":\n",
      "        \"\"\"Executes the entire DataFrame and materializes the results.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            num_preview_rows: Number of rows to preview. Defaults to 8.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: DataFrame with materialized results.\n",
      "        \"\"\"\n",
      "        plan_time_start = _utc_now()\n",
      "        self._materialize_results()\n",
      "        plan_time_end = _utc_now()\n",
      "        self._broadcast_query_plan(plan_time_start, plan_time_end)\n",
      "        assert self._result is not None\n",
      "        dataframe_len = len(self._result)\n",
      "        if num_preview_rows is not None:\n",
      "            self._num_preview_rows = num_preview_rows\n",
      "        else:\n",
      "            self._num_preview_rows = dataframe_len\n",
      "        return self\n",
      "\n",
      "    def _construct_show_display(self, n: int) -> \"DataFrameDisplay\":\n",
      "        \"\"\"Helper for .show() which will construct the underlying DataFrameDisplay object.\"\"\"\n",
      "        preview_partition = self._preview.preview_partition\n",
      "        total_rows = self._preview.dataframe_num_rows\n",
      "\n",
      "        # Truncate n to the length of the DataFrame, if we have it.\n",
      "        if total_rows is not None and n > total_rows:\n",
      "            n = total_rows\n",
      "\n",
      "        # Construct the PreviewPartition\n",
      "        if preview_partition is None or len(preview_partition) < n:\n",
      "            # Preview partition doesn't exist or doesn't contain enough rows, so we need to compute a\n",
      "            # new one from scratch.\n",
      "            builder = self._builder.limit(n, eager=True)\n",
      "\n",
      "            # Iteratively retrieve partitions until enough data has been materialized\n",
      "            tables = []\n",
      "            seen = 0\n",
      "            for table in get_context().get_or_create_runner().run_iter_tables(builder, results_buffer_size=1):\n",
      "                tables.append(table)\n",
      "                seen += len(table)\n",
      "                if seen >= n:\n",
      "                    break\n",
      "\n",
      "            preview_partition = MicroPartition.concat(tables)\n",
      "            if len(preview_partition) > n:\n",
      "                preview_partition = preview_partition.slice(0, n)\n",
      "            elif len(preview_partition) < n:\n",
      "                # Iterator short-circuited before reaching n, so we know that we have the full DataFrame.\n",
      "                total_rows = n = len(preview_partition)\n",
      "            preview = DataFramePreview(\n",
      "                preview_partition=preview_partition,\n",
      "                dataframe_num_rows=total_rows,\n",
      "            )\n",
      "        elif len(preview_partition) > n:\n",
      "            # Preview partition is cached but has more rows that we need, so use the appropriate slice.\n",
      "            truncated_preview_partition = preview_partition.slice(0, n)\n",
      "            preview = DataFramePreview(\n",
      "                preview_partition=truncated_preview_partition,\n",
      "                dataframe_num_rows=total_rows,\n",
      "            )\n",
      "        else:\n",
      "            assert len(preview_partition) == n\n",
      "            # Preview partition is cached and has exactly the number of rows that we need, so use it directly.\n",
      "            preview = self._preview\n",
      "\n",
      "        return DataFrameDisplay(preview, self.schema(), num_rows=n)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def show(self, n: int = 8) -> None:\n",
      "        \"\"\"Executes enough of the DataFrame in order to display the first ``n`` rows.\n",
      "\n",
      "        If IPython is installed, this will use IPython's `display` utility to pretty-print in a\n",
      "        notebook/REPL environment. Otherwise, this will fall back onto a naive Python `print`.\n",
      "\n",
      "        .. NOTE::\n",
      "            This call is **blocking** and will execute the DataFrame when called\n",
      "\n",
      "        Args:\n",
      "            n: number of rows to show. Defaults to 8.\n",
      "        \"\"\"\n",
      "        dataframe_display = self._construct_show_display(n)\n",
      "\n",
      "        try:\n",
      "            from IPython.display import display\n",
      "\n",
      "            display(dataframe_display, clear=True)\n",
      "        except ImportError:\n",
      "            print(dataframe_display)\n",
      "        return None\n",
      "\n",
      "    def __len__(self):\n",
      "        \"\"\"Returns the count of rows when dataframe is materialized.\n",
      "\n",
      "        If dataframe is not materialized yet, raises a runtime error.\n",
      "\n",
      "        Returns:\n",
      "            int: count of rows.\n",
      "\n",
      "        \"\"\"\n",
      "        if self._result is not None:\n",
      "            return len(self._result)\n",
      "\n",
      "        message = (\n",
      "            \"Cannot call len() on an unmaterialized dataframe:\"\n",
      "            \" either materialize your dataframe with df.collect() first before calling len(),\"\n",
      "            \" or use `df.count_rows()` instead which will calculate the total number of rows.\"\n",
      "        )\n",
      "        raise RuntimeError(message)\n",
      "\n",
      "    def __contains__(self, col_name: str) -> bool:\n",
      "        \"\"\"Returns whether the column exists in the dataframe.\n",
      "\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> df = daft.from_pydict({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8, 9]})\n",
      "            >>> \"x\" in df\n",
      "            True\n",
      "\n",
      "        Args:\n",
      "            col_name (str): column name\n",
      "\n",
      "        Returns:\n",
      "            bool: whether the column exists in the dataframe.\n",
      "        \"\"\"\n",
      "        return col_name in self.column_names\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_pandas(self, coerce_temporal_nanoseconds: bool = False) -> \"pandas.DataFrame\":\n",
      "        \"\"\"Converts the current DataFrame to a `pandas DataFrame <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>`__.\n",
      "\n",
      "        If results have not computed yet, collect will be called.\n",
      "\n",
      "        Args:\n",
      "            coerce_temporal_nanoseconds (bool): Whether to coerce temporal columns to nanoseconds. Only applicable to pandas version >= 2.0 and pyarrow version >= 13.0.0. Defaults to False. See `pyarrow.Table.to_pandas <https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_pandas>`__ for more information.\n",
      "\n",
      "        Returns:\n",
      "            pandas.DataFrame: `pandas DataFrame <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>`__ converted from a Daft DataFrame\n",
      "\n",
      "            .. NOTE::\n",
      "                This call is **blocking** and will execute the DataFrame when called\n",
      "        \"\"\"\n",
      "        self.collect()\n",
      "        result = self._result\n",
      "        assert result is not None\n",
      "\n",
      "        pd_df = result.to_pandas(\n",
      "            schema=self._builder.schema(),\n",
      "            coerce_temporal_nanoseconds=coerce_temporal_nanoseconds,\n",
      "        )\n",
      "        return pd_df\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_arrow(self) -> \"pyarrow.Table\":\n",
      "        \"\"\"Converts the current DataFrame to a `pyarrow Table <https://arrow.apache.org/docs/python/generated/pyarrow.Table.html>`__.\n",
      "\n",
      "        If results have not computed yet, collect will be called.\n",
      "\n",
      "        Returns:\n",
      "            pyarrow.Table: `pyarrow Table <https://arrow.apache.org/docs/python/generated/pyarrow.Table.html>`__ converted from a Daft DataFrame\n",
      "\n",
      "            .. NOTE::\n",
      "                This call is **blocking** and will execute the DataFrame when called\n",
      "        \"\"\"\n",
      "        import pyarrow as pa\n",
      "\n",
      "        arrow_rb_iter = self.to_arrow_iter(results_buffer_size=None)\n",
      "        return pa.Table.from_batches(arrow_rb_iter, schema=self.schema().to_pyarrow_schema())\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_pydict(self) -> Dict[str, List[Any]]:\n",
      "        \"\"\"Converts the current DataFrame to a python dictionary. The dictionary contains Python lists of Python objects for each column.\n",
      "\n",
      "        If results have not computed yet, collect will be called.\n",
      "\n",
      "        Returns:\n",
      "            dict[str, list[Any]]: python dict converted from a Daft DataFrame\n",
      "\n",
      "            .. NOTE::\n",
      "                This call is **blocking** and will execute the DataFrame when called\n",
      "        \"\"\"\n",
      "        self.collect()\n",
      "        result = self._result\n",
      "        assert result is not None\n",
      "        return result.to_pydict()\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_pylist(self) -> List[Any]:\n",
      "        \"\"\"Converts the current Dataframe into a python list.\n",
      "\n",
      "        .. WARNING::\n",
      "\n",
      "            This is a convenience method over :meth:`DataFrame.iter_rows() <daft.DataFrame.iter_rows>`. Users should prefer using `.iter_rows()` directly instead for lower memory utilization if they are streaming rows out of a DataFrame and don't require full materialization of the Python list.\n",
      "\n",
      "        .. seealso::\n",
      "            :meth:`df.iter_rows() <daft.DataFrame.iter_rows>`: streaming iterator over individual rows in a DataFrame\n",
      "        Example:\n",
      "            >>> import daft\n",
      "            >>> from daft import col\n",
      "            >>> df = daft.from_pydict({\"a\": [1, 2, 3, 4], \"b\": [2, 4, 3, 1]})\n",
      "            >>> print(df.to_pylist())\n",
      "            [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}, {'a': 3, 'b': 3}, {'a': 4, 'b': 1}]\n",
      "\n",
      "        Returns:\n",
      "            List[dict[str, Any]]: List of python dict objects.\n",
      "        \"\"\"\n",
      "        return list(self.iter_rows())\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_torch_map_dataset(self) -> \"torch.utils.data.Dataset\":\n",
      "        \"\"\"Convert the current DataFrame into a map-style `Torch Dataset <https://pytorch.org/docs/stable/data.html#map-style-datasets>`__ for use with PyTorch.\n",
      "\n",
      "        This method will materialize the entire DataFrame and block on completion.\n",
      "\n",
      "        Items will be returned in pydict format: a dict of `{\"column name\": value}` for each row in the data.\n",
      "\n",
      "        .. NOTE::\n",
      "            If you do not need random access, you may get better performance out of an IterableDataset,\n",
      "            which streams data items in as soon as they are ready and does not block on full materialization.\n",
      "\n",
      "        .. NOTE::\n",
      "            This method returns results locally.\n",
      "            For distributed training, you may want to use ``DataFrame.to_ray_dataset()``.\n",
      "        \"\"\"\n",
      "        from daft.dataframe.to_torch import DaftTorchDataset\n",
      "\n",
      "        return DaftTorchDataset(self.to_pydict(), len(self))\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_torch_iter_dataset(self) -> \"torch.utils.data.IterableDataset\":\n",
      "        \"\"\"Convert the current DataFrame into a `Torch IterableDataset <https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset>`__ for use with PyTorch.\n",
      "\n",
      "        Begins execution of the DataFrame if it is not yet executed.\n",
      "\n",
      "        Items will be returned in pydict format: a dict of `{\"column name\": value}` for each row in the data.\n",
      "\n",
      "        .. NOTE::\n",
      "            The produced dataset is meant to be used with the single-process DataLoader,\n",
      "            and does not support data sharding hooks for multi-process data loading.\n",
      "\n",
      "            Do keep in mind that Daft is already using multithreading or multiprocessing under the hood\n",
      "            to compute the data stream that feeds this dataset.\n",
      "\n",
      "        .. NOTE::\n",
      "            This method returns results locally.\n",
      "            For distributed training, you may want to use ``DataFrame.to_ray_dataset()``.\n",
      "        \"\"\"\n",
      "        from daft.dataframe.to_torch import DaftTorchIterableDataset\n",
      "\n",
      "        return DaftTorchIterableDataset(self)\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_ray_dataset(self) -> \"ray.data.dataset.DataSet\":\n",
      "        \"\"\"Converts the current DataFrame to a `Ray Dataset <https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset>`__ which is useful for running distributed ML model training in Ray.\n",
      "\n",
      "        .. NOTE::\n",
      "            This function can only work if Daft is running using the RayRunner\n",
      "\n",
      "        Returns:\n",
      "            ray.data.dataset.DataSet: `Ray dataset <https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset>`__\n",
      "        \"\"\"\n",
      "        from daft.runners.ray_runner import RayPartitionSet\n",
      "\n",
      "        self.collect()\n",
      "        partition_set = self._result\n",
      "        assert partition_set is not None\n",
      "        if not isinstance(partition_set, RayPartitionSet):\n",
      "            raise ValueError(\"Cannot convert to Ray Dataset if not running on Ray backend\")\n",
      "        return partition_set.to_ray_dataset()\n",
      "\n",
      "    @classmethod\n",
      "    def _from_ray_dataset(cls, ds: \"ray.data.dataset.DataSet\") -> \"DataFrame\":\n",
      "        \"\"\"Creates a DataFrame from a `Ray Dataset <https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset>`__.\"\"\"\n",
      "        from ray.exceptions import RayTaskError\n",
      "\n",
      "        context = get_context()\n",
      "        if context.get_or_create_runner().name != \"ray\":\n",
      "            raise ValueError(\"Daft needs to be running on the Ray Runner for this operation\")\n",
      "\n",
      "        from daft.runners.ray_runner import RayRunnerIO\n",
      "\n",
      "        ray_runner_io = context.get_or_create_runner().runner_io()\n",
      "        assert isinstance(ray_runner_io, RayRunnerIO)\n",
      "\n",
      "        partition_set, schema = ray_runner_io.partition_set_from_ray_dataset(ds)\n",
      "        cache_entry = context.get_or_create_runner().put_partition_set_into_cache(partition_set)\n",
      "        try:\n",
      "            size_bytes = partition_set.size_bytes()\n",
      "        except RayTaskError as e:\n",
      "            import pyarrow as pa\n",
      "            from packaging.version import parse\n",
      "\n",
      "            if \"extension<arrow.fixed_shape_tensor>\" in str(e) and parse(pa.__version__) < parse(\"13.0.0\"):\n",
      "                raise ValueError(\n",
      "                    f\"Reading Ray Dataset tensors is only supported with PyArrow >= 13.0.0, found {pa.__version__}. See this issue for more information: https://github.com/apache/arrow/pull/35933\"\n",
      "                ) from e\n",
      "            raise e\n",
      "\n",
      "        num_rows = len(partition_set)\n",
      "        assert size_bytes is not None, \"In-memory data should always have non-None size in bytes\"\n",
      "        builder = LogicalPlanBuilder.from_in_memory_scan(\n",
      "            cache_entry,\n",
      "            schema=schema,\n",
      "            num_partitions=partition_set.num_partitions(),\n",
      "            size_bytes=size_bytes,\n",
      "            num_rows=num_rows,\n",
      "        )\n",
      "        df = cls(builder)\n",
      "        df._result_cache = cache_entry\n",
      "\n",
      "        # build preview\n",
      "        num_preview_rows = context.daft_execution_config.num_preview_rows\n",
      "        dataframe_num_rows = len(df)\n",
      "        if dataframe_num_rows > num_preview_rows:\n",
      "            preview_results, _ = ray_runner_io.partition_set_from_ray_dataset(ds.limit(num_preview_rows))\n",
      "        else:\n",
      "            preview_results = partition_set\n",
      "\n",
      "        # set preview\n",
      "        preview_partition = preview_results._get_merged_micropartition()\n",
      "        df._preview = DataFramePreview(\n",
      "            preview_partition=preview_partition,\n",
      "            dataframe_num_rows=dataframe_num_rows,\n",
      "        )\n",
      "        return df\n",
      "\n",
      "    @DataframePublicAPI\n",
      "    def to_dask_dataframe(\n",
      "        self,\n",
      "        meta: Union[\n",
      "            \"pandas.DataFrame\",\n",
      "            \"pandas.Series\",\n",
      "            Dict[str, Any],\n",
      "            Iterable[Any],\n",
      "            Tuple[Any],\n",
      "            None,\n",
      "        ] = None,\n",
      "    ) -> \"dask.DataFrame\":\n",
      "        \"\"\"Converts the current Daft DataFrame to a Dask DataFrame.\n",
      "\n",
      "        The returned Dask DataFrame will use `Dask-on-Ray <https://docs.ray.io/en/latest/ray-more-libs/dask-on-ray.html>`__\n",
      "        to execute operations on a Ray cluster.\n",
      "\n",
      "        .. NOTE::\n",
      "            This function can only work if Daft is running using the RayRunner.\n",
      "\n",
      "        Args:\n",
      "            meta: An empty pandas `DataFrame <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html>`__ or `Series <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html>`__ that matches the dtypes and column\n",
      "                names of the stream. This metadata is necessary for many algorithms in\n",
      "                dask dataframe to work. For ease of use, some alternative inputs are\n",
      "                also available. Instead of a DataFrame, a dict of ``{name: dtype}`` or\n",
      "                iterable of ``(name, dtype)`` can be provided (note that the order of\n",
      "                the names should match the order of the columns). Instead of a series, a\n",
      "                tuple of ``(name, dtype)`` can be used.\n",
      "                By default, this will be inferred from the underlying Daft DataFrame schema,\n",
      "                with this argument supplying an optional override.\n",
      "\n",
      "        Returns:\n",
      "            dask.DataFrame: A Dask DataFrame stored on a Ray cluster.\n",
      "        \"\"\"\n",
      "        from daft.runners.ray_runner import RayPartitionSet\n",
      "\n",
      "        self.collect()\n",
      "        partition_set = self._result\n",
      "        assert partition_set is not None\n",
      "        # TODO(Clark): Support Dask DataFrame conversion for the local runner if\n",
      "        # Dask is using a non-distributed scheduler.\n",
      "        if not isinstance(partition_set, RayPartitionSet):\n",
      "            raise ValueError(\"Cannot convert to Dask DataFrame if not running on Ray backend\")\n",
      "        return partition_set.to_dask_dataframe(meta)\n",
      "\n",
      "    @classmethod\n",
      "    @DataframePublicAPI\n",
      "    def _from_dask_dataframe(cls, ddf: \"dask.DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Creates a Daft DataFrame from a Dask DataFrame.\"\"\"\n",
      "        # TODO(Clark): Support Dask DataFrame conversion for the local runner if\n",
      "        # Dask is using a non-distributed scheduler.\n",
      "        context = get_context()\n",
      "        if context.get_or_create_runner().name != \"ray\":\n",
      "            raise ValueError(\"Daft needs to be running on the Ray Runner for this operation\")\n",
      "\n",
      "        from daft.runners.ray_runner import RayRunnerIO\n",
      "\n",
      "        ray_runner_io = context.get_or_create_runner().runner_io()\n",
      "        assert isinstance(ray_runner_io, RayRunnerIO)\n",
      "\n",
      "        partition_set, schema = ray_runner_io.partition_set_from_dask_dataframe(ddf)\n",
      "        cache_entry = context.get_or_create_runner().put_partition_set_into_cache(partition_set)\n",
      "        size_bytes = partition_set.size_bytes()\n",
      "        num_rows = len(partition_set)\n",
      "        assert size_bytes is not None, \"In-memory data should always have non-None size in bytes\"\n",
      "        builder = LogicalPlanBuilder.from_in_memory_scan(\n",
      "            cache_entry,\n",
      "            schema=schema,\n",
      "            num_partitions=partition_set.num_partitions(),\n",
      "            size_bytes=size_bytes,\n",
      "            num_rows=num_rows,\n",
      "        )\n",
      "\n",
      "        df = cls(builder)\n",
      "        df._result_cache = cache_entry\n",
      "\n",
      "        # build preview\n",
      "        num_preview_rows = context.daft_execution_config.num_preview_rows\n",
      "        dataframe_num_rows = len(df)\n",
      "        if dataframe_num_rows > num_preview_rows:\n",
      "            preview_results, _ = ray_runner_io.partition_set_from_dask_dataframe(ddf.loc[: num_preview_rows - 1])\n",
      "        else:\n",
      "            preview_results = partition_set\n",
      "\n",
      "        # set preview\n",
      "        preview_partition = preview_results._get_merged_micropartition()\n",
      "        df._preview = DataFramePreview(\n",
      "            preview_partition=preview_partition,\n",
      "            dataframe_num_rows=dataframe_num_rows,\n",
      "        )\n",
      "        return df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo  = inspect.getsource(daft.DataFrame)\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ast.Module object at 0x13780bb50>\n"
     ]
    }
   ],
   "source": [
    "# We can't directly parse a class object with ast.parse\n",
    "# Instead, we need to get the source code of the DataFrame class\n",
    "data = inspect.is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXError",
     "evalue": "Input is not a known data type for conversion.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m nx\u001b[38;5;241m.\u001b[39mdraw(G, with_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/git/vangelis/internal/work/.venv/lib/python3.10/site-packages/networkx/classes/graph.py:378\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, incoming_graph_data, **attr)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# attempt to load graph with data\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incoming_graph_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[43mconvert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_networkx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincoming_graph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# load graph attributes (must be after convert)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(attr)\n",
      "File \u001b[0;32m~/git/vangelis/internal/work/.venv/lib/python3.10/site-packages/networkx/convert.py:184\u001b[0m, in \u001b[0;36mto_networkx_graph\u001b[0;34m(data, create_using, multigraph_input)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not a valid edge list\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not a known data type for conversion.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNetworkXError\u001b[0m: Input is not a known data type for conversion."
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.Graph(data)\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
